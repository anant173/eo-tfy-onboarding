{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8534fad7",
   "metadata": {},
   "source": [
    "# Advanced Retrieval Augmented Generation with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af284c2",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "efea333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7fc80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, getpass, warnings\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "27dc7c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38ad899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Groq API Key\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"GROQ API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2fbf0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Hugging Face token (as we will be using some of Hugging Face's functionalities\n",
    "from huggingface_hub.hf_api import HfFolder\n",
    "HfFolder.save_token(os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d96f6d",
   "metadata": {},
   "source": [
    "## Defining Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ae83c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question\n",
    "question = \"What is the PGP AI & DS at Jio Institute all about?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a708729",
   "metadata": {},
   "source": [
    "### Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3371ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model_name = \"llama-3.1-8b-instant\"\n",
    "llm = init_chat_model(model_name, model_provider=\"groq\") #Other Llama alternatives available are llama3-8b-8192, llama-3.3-70b-versatile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a333d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7fc938a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 667b0c1932bc: 100% ▕██████████████████▏ 4.9 GB                         \u001b[K\n",
      "pulling 948af2743fc7: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 0ba8f0e314b4: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 455f34728c9b: 100% ▕██████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7b91c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings_model = OllamaEmbeddings(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434b6e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bced664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a1fa070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a vector store\n",
    "vector_store_chroma = Chroma(\n",
    "    collection_name=\"session-4\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba474b6",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ad623",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f14ba019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 #import Beautiful Soup\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc57b7f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 4210\n"
     ]
    }
   ],
   "source": [
    "# Only keep the main content from the full HTML\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"node__content clearfix\", \"col-md-9 pl-lg-5\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.jioinstitute.edu.in/about/\",\n",
    "    \"https://www.jioinstitute.edu.in/academics/artificial-intelligence-data-science\"),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 2\n",
    "\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13e0a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Post-processing to clean up excessive newlines and whitespace ---\n",
    "for i, doc in enumerate(docs):\n",
    "    if doc.page_content:\n",
    "        # 1. Replace multiple newlines with a single newline\n",
    "        cleaned_content = re.sub(r'\\n\\s*\\n', '\\n\\n', doc.page_content)\n",
    "        # 2. Replace multiple spaces with a single space\n",
    "        cleaned_content = re.sub(r' {2,}', ' ', cleaned_content)\n",
    "        # 3. Strip leading/trailing whitespace from each line\n",
    "        cleaned_content = '\\n'.join([line.strip() for line in cleaned_content.split('\\n')])\n",
    "        # 4. Remove leading/trailing whitespace from the whole string\n",
    "        cleaned_content = cleaned_content.strip()\n",
    "    \n",
    "        docs[i].page_content = cleaned_content\n",
    "# --- End of post-processing ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67fba12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 3548\n",
      "\n",
      "--- Cleaned Content Snippet ---\n",
      "About Us\n",
      "\n",
      "Jio Institute is a multidisciplinary higher education institute set up as a philanthropic initiative by the Reliance Group. The Institute is dedicated to the pursuit of excellence by bringing together global scholars and thought leaders and providing an enriching student experience through world-class education, and a culture of research and innovation.\n",
      "\n",
      "Our Story\n",
      "Pursuit of excellence in academics, research and innovation.\n",
      "We stand at the confluence of the best higher education practices from India and the world. The institute aims to nurture students’ aspirations, and provide a platform to their entrepreneurial spirit.\n",
      "\n",
      "Read more\n",
      "\n",
      "Our Vision\n",
      "In sync with global aspirations. In step with changing times.\n",
      "We envisage to be a world-class higher education institute through our multi-disciplinary academic programmes, robust research endeavours and a culture of innovation and entrepreneurship.\n",
      "\n",
      "Read more\n",
      "\n",
      "Growth Plan\n",
      "Well thought out growth strategy.\n",
      "In the future, we envisage to \n"
     ]
    }
   ],
   "source": [
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "print(\"\\n--- Cleaned Content Snippet ---\")\n",
    "print(docs[0].page_content[:1000]) # Print a snippet to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bce8a9f",
   "metadata": {},
   "source": [
    "### Splitting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a951515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 62 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "badf635b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Split0 ---\n",
      "\n",
      "Secure Deployment : VPC | On-Prem | Air-GappedAI Gateway: Fast, Scalable, Enterprise-ReadyEnterprise-Ready AI Gateway for secure, high-performance LLM access, observability, and orchestration.See detailed pricingGet Started for FreeAI Gateway: Unified LLM API AccessSimplify your GenAI stack with a single AI Gateway that integrates all major models.\n",
      "\n",
      "Connect to OpenAI, Claude, Gemini, Groq, Mistral, and 250+ LLMs through one AI Gateway API\n",
      "\n",
      "Use the platform to support chat, completion, embedding, and reranking model types.\n",
      "\n",
      "Centralize API key management and team authentication in one place.\n",
      "\n",
      "Orchestrate multi-model workloads seamlessly through your infrastructure.Read MoreAI Gateway Observability\n",
      "\n",
      "Monitor token usage, latency, error rates, and request volumes across your system.\n",
      "\n",
      "Store and inspect full request/response logs centrally to ensure compliance and simplify debugging.\n",
      "\n",
      "Tag traffic with metadata like user ID, team, or environment to gain granular insights.\n",
      "\n",
      "--- Split1 ---\n",
      "\n",
      "Store and inspect full request/response logs centrally to ensure compliance and simplify debugging.\n",
      "\n",
      "Tag traffic with metadata like user ID, team, or environment to gain granular insights.\n",
      "\n",
      "Filter logs and metrics by model, team, or geography to quickly pinpoint root causes and accelerate resolution.Read MoreQuota & Access Control via AI GatewayEnforce governance, control costs, and reduce risk with consistent policy management.\n",
      "\n",
      "Apply rate limits per user, service, or endpoint.\n",
      "\n",
      "Set cost-based or token-based quotas using metadata filters.\n",
      "\n",
      "Use role-based access control (RBAC) to isolate and manage usage.\n",
      "\n",
      "Govern service accounts and agent workloads at scale through centralized rules.Read MoreEnsuring predictable usage, strong access boundaries, and scalable team-level governance for your GenAI infrastructure.Low-Latency Inference Run your most performance-sensitive workloads through a high-speed infrastructure.\n",
      "\n",
      "Achieve sub-3ms internal latency even under enterprise-scale workloads.\n",
      "\n",
      "--- Split2 ---\n",
      "\n",
      "Achieve sub-3ms internal latency even under enterprise-scale workloads.\n",
      "\n",
      "Scale seamlessly to manage burst traffic and high-throughput workloads.\n",
      "\n",
      "Deliver predictable response times for real-time chat, RAG, and AI assistants.\n",
      "\n",
      "Place deployments close to inference layers to minimize latency and eliminate network lag.Read MorePlace the AI Gateway directly in your production inference path — its low-latency architecture ensures no performance tradeoffs.AI Gateway Routing & FallbacksEnsure reliability, even during model failures, with smart AI Gateway traffic controls.\n",
      "\n",
      "Supports latency-based routing to the fastest available LLM.\n",
      "\n",
      "Distribute traffic intelligently using weighted load balancing for reliability and scale.\n",
      "\n",
      "Automatically fallback to secondary models when a request fails.\n",
      "\n",
      "--- Split3 ---\n",
      "\n",
      "Distribute traffic intelligently using weighted load balancing for reliability and scale.\n",
      "\n",
      "Automatically fallback to secondary models when a request fails.\n",
      "\n",
      "Use geo-aware routing to meet regional compliance and availability needs.Read MoreThis system ensures you never go offline, even when individual models face downtime or spike in latency.Serve Self-Hosted Models Expose open-source models with full control.\n",
      "\n",
      "Deploy LLaMA, Mistral, Falcon, and more with zero SDK changes.\n",
      "\n",
      "Full compatibility with vLLM, SGLang, KServe, and Triton.\n",
      "\n",
      "Streamline operations with Helm-based management of autoscaling, GPU scheduling, and deployments\n",
      "\n",
      "Run your own models in VPC, hybrid, or air-gapped environments.Read MoreAI Gateway + MCP IntegrationPower secure agent workflows through the AI Gateway’s native MCP support.\n",
      "\n",
      "Connect enterprise tools like Slack, GitHub, Confluence, and Datadog.\n",
      "\n",
      "Easily register internal MCP Servers with minimal setup required.\n",
      "\n",
      "--- Split4 ---\n",
      "\n",
      "Connect enterprise tools like Slack, GitHub, Confluence, and Datadog.\n",
      "\n",
      "Easily register internal MCP Servers with minimal setup required.\n",
      "\n",
      "Apply OAuth2, RBAC, and metadata policies to every tool call.Read MoreAI Gateway Guardrails\n",
      "\n",
      "Seamlessly enforce your own safety guardrails, including PII filtering and toxicity detection\n",
      "\n",
      "Customize the AI Gateway with guardrails tailored to your compliance and safety needsRead More\n",
      "\n",
      "--- Split5 ---\n",
      "\n",
      "Orchestrate Agentic AI with AI GatewayEnable intelligent multi-step reasoning, tool usage, and memory with full control and visibility across your AI agents and workflows.AI GatewayManage agent memory, tool orchestration, and action planning through a centralized protocol that supports complex, context-aware workflows.Learn More\n",
      "\n",
      "MCP & Agents RegistryMaintain a structured, discoverable registry of tools and APIs accessible to agents, complete with schema validation and access control.Learn More\n",
      "\n",
      "Prompt Lifecycle ManagementVersion, manage, and monitor prompts to ensure high-quality, repeatable behavior across agents and use cases.Learn More\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_splits)):\n",
    "    print(f\"\\n--- Split{i} ---\\n\")\n",
    "    print(all_splits[i].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709da37",
   "metadata": {},
   "source": [
    "### Storing in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23c9aedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['71535f94-9ea0-408f-b161-82511c4f2f35',\n",
       " 'bf8ae97d-95b6-4488-b7c1-c01e56d1b477',\n",
       " '22d7a904-19e6-46b7-87f7-c0cdfca18d42',\n",
       " 'e0595d54-3eda-4bcd-98a2-9a4dafc26d49',\n",
       " 'f293ff4c-e6db-4c63-86ef-27c6119ee3b5',\n",
       " 'af769fe8-3114-4311-8ed6-ed3e10073295']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add first 10 chunks to vector DB\n",
    "uuids = [str(uuid4()) for _ in range(len(all_splits))] #Universally unique identifier\n",
    "vector_store_chroma.add_documents(documents=all_splits[:10], ids=uuids[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df8746",
   "metadata": {},
   "source": [
    "## Advanced Retrieval and Reranking Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ffbec",
   "metadata": {},
   "source": [
    "### Multi Query Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a25b5e",
   "metadata": {},
   "source": [
    "Retrieval may produce different results with subtle changes in query wording, or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\n",
    "\n",
    "The [`MultiQueryRetriever`](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html) automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea924303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "import logging # Set logging for the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cda934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize retriever\n",
    "retriever = vector_store_chroma.as_retriever(search_type=\"similarity\",\n",
    "                                                search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3b475b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mq_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, \n",
    "    llm=llm,\n",
    "    include_original=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20c594f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "# so we can see what queries are generated by the LLM\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffeb6dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', \"What is TrueFoundry's mission and values?\", ' ', 'What does TrueFoundry specialize in or offer to its users?', 'What key aspects or features of TrueFoundry should I know about?', 'These alternative questions aim to capture different nuances of the original question, allowing the vector database to return relevant documents that may not have been retrieved by a single search query.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='af769fe8-3114-4311-8ed6-ed3e10073295', metadata={'start_index': 0, 'source': 'https://www.truefoundry.com'}, page_content='Orchestrate Agentic AI with AI GatewayEnable intelligent multi-step reasoning, tool usage, and memory with full control and visibility across your AI agents and workflows.AI GatewayManage agent memory, tool orchestration, and action planning through a centralized protocol that supports complex, context-aware workflows.Learn More\\n\\nMCP & Agents RegistryMaintain a structured, discoverable registry of tools and APIs accessible to agents, complete with schema validation and access control.Learn More\\n\\nPrompt Lifecycle ManagementVersion, manage, and monitor prompts to ensure high-quality, repeatable behavior across agents and use cases.Learn More'),\n",
       " Document(id='bf8ae97d-95b6-4488-b7c1-c01e56d1b477', metadata={'start_index': 790, 'source': 'https://www.truefoundry.com/ai-gateway'}, page_content='Store and inspect full request/response logs centrally to ensure compliance and simplify debugging.\\n\\nTag traffic with metadata like user ID, team, or environment to gain granular insights.\\n\\nFilter logs and metrics by model, team, or geography to quickly pinpoint root causes and accelerate resolution.Read MoreQuota & Access Control via AI GatewayEnforce governance, control costs, and reduce risk with consistent policy management.\\n\\nApply rate limits per user, service, or endpoint.\\n\\nSet cost-based or token-based quotas using metadata filters.\\n\\nUse role-based access control (RBAC) to isolate and manage usage.\\n\\nGovern service accounts and agent workloads at scale through centralized rules.Read MoreEnsuring predictable usage, strong access boundaries, and scalable team-level governance for your GenAI infrastructure.Low-Latency Inference\\xa0Run your most performance-sensitive workloads through a high-speed infrastructure.\\n\\nAchieve sub-3ms internal latency even under enterprise-scale workloads.'),\n",
       " Document(id='22d7a904-19e6-46b7-87f7-c0cdfca18d42', metadata={'source': 'https://www.truefoundry.com/ai-gateway', 'start_index': 1717}, page_content='Achieve sub-3ms internal latency even under enterprise-scale workloads.\\n\\nScale seamlessly to manage burst traffic and high-throughput workloads.\\n\\nDeliver predictable response times for real-time chat, RAG, and AI assistants.\\n\\nPlace deployments close to inference layers to minimize latency and eliminate network lag.Read MorePlace the AI Gateway directly in your production inference path — its low-latency architecture ensures no performance tradeoffs.AI Gateway Routing & FallbacksEnsure reliability, even during model failures, with smart AI Gateway traffic controls.\\n\\nSupports latency-based routing to the fastest available LLM.\\n\\nDistribute traffic intelligently using weighted load balancing for reliability and scale.\\n\\nAutomatically fallback to secondary models when a request fails.'),\n",
       " Document(id='e0595d54-3eda-4bcd-98a2-9a4dafc26d49', metadata={'start_index': 2351, 'source': 'https://www.truefoundry.com/ai-gateway'}, page_content='Distribute traffic intelligently using weighted load balancing for reliability and scale.\\n\\nAutomatically fallback to secondary models when a request fails.\\n\\nUse geo-aware routing to meet regional compliance and availability needs.Read MoreThis system ensures you never go offline, even when individual models face downtime or spike in latency.Serve Self-Hosted Models Expose open-source models with full control.\\n\\nDeploy LLaMA, Mistral, Falcon, and more with zero SDK changes.\\n\\nFull compatibility with vLLM, SGLang, KServe, and Triton.\\n\\nStreamline operations with Helm-based management of autoscaling, GPU scheduling, and deployments\\n\\nRun your own models in VPC, hybrid, or air-gapped environments.Read MoreAI Gateway + MCP IntegrationPower secure agent workflows through the AI Gateway’s native MCP support.\\n\\nConnect enterprise tools like Slack, GitHub, Confluence, and Datadog.\\n\\nEasily register internal MCP Servers with minimal setup required.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = mq_retriever.invoke(question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f323a",
   "metadata": {},
   "source": [
    "### Chained Retrieval with Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95731a6a",
   "metadata": {},
   "source": [
    "This strategy uses a chain of multiple retrievers sequentially to get to the most relevant documents. The following is the flow:\n",
    "\n",
    "*Similarity Retrieval → Reranker Model Retrieval*\n",
    "\n",
    "**What are rerankers?**\n",
    "\n",
    "- Rerankers are fine-tuned cross-encoder transformer models\n",
    "- These models take in a pair of documents (Query, Document) and return back a relevance score\n",
    "- Models fine-tuned on more pairs and released recently will usually be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d61679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3deb415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever 1 - simple cosine distance based retriever\n",
    "retriever = vector_store_chroma.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0a3ad32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3364b8ad3a4c2b9b1574d8d81c43f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/771 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44e1464b88f41918c64154ac5f29458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237e3d33d3b948f1addb416593970714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa09c7d9c1ac4ce2a6fbf743b9aafa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71c05afc7c64981af50d3f750088f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23eb5ce1409a499385548ae8e763c00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d01bd9ff9c410ebcb328d54d1d5822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download an open-source reranker model - cross-encoder/qnli-electra-base\n",
    "reranker = HuggingFaceCrossEncoder(model_name=\"cross-encoder/qnli-electra-base\")\n",
    "reranker_compressor = CrossEncoderReranker(model=reranker, top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1a297fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever 2 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
    "final_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker_compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8407d9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='bf8ae97d-95b6-4488-b7c1-c01e56d1b477', metadata={'source': 'https://www.truefoundry.com/ai-gateway', 'start_index': 790}, page_content='Store and inspect full request/response logs centrally to ensure compliance and simplify debugging.\\n\\nTag traffic with metadata like user ID, team, or environment to gain granular insights.\\n\\nFilter logs and metrics by model, team, or geography to quickly pinpoint root causes and accelerate resolution.Read MoreQuota & Access Control via AI GatewayEnforce governance, control costs, and reduce risk with consistent policy management.\\n\\nApply rate limits per user, service, or endpoint.\\n\\nSet cost-based or token-based quotas using metadata filters.\\n\\nUse role-based access control (RBAC) to isolate and manage usage.\\n\\nGovern service accounts and agent workloads at scale through centralized rules.Read MoreEnsuring predictable usage, strong access boundaries, and scalable team-level governance for your GenAI infrastructure.Low-Latency Inference\\xa0Run your most performance-sensitive workloads through a high-speed infrastructure.\\n\\nAchieve sub-3ms internal latency even under enterprise-scale workloads.'),\n",
       " Document(id='e0595d54-3eda-4bcd-98a2-9a4dafc26d49', metadata={'source': 'https://www.truefoundry.com/ai-gateway', 'start_index': 2351}, page_content='Distribute traffic intelligently using weighted load balancing for reliability and scale.\\n\\nAutomatically fallback to secondary models when a request fails.\\n\\nUse geo-aware routing to meet regional compliance and availability needs.Read MoreThis system ensures you never go offline, even when individual models face downtime or spike in latency.Serve Self-Hosted Models Expose open-source models with full control.\\n\\nDeploy LLaMA, Mistral, Falcon, and more with zero SDK changes.\\n\\nFull compatibility with vLLM, SGLang, KServe, and Triton.\\n\\nStreamline operations with Helm-based management of autoscaling, GPU scheduling, and deployments\\n\\nRun your own models in VPC, hybrid, or air-gapped environments.Read MoreAI Gateway + MCP IntegrationPower secure agent workflows through the AI Gateway’s native MCP support.\\n\\nConnect enterprise tools like Slack, GitHub, Confluence, and Datadog.\\n\\nEasily register internal MCP Servers with minimal setup required.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = final_retriever.invoke(question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f397cd",
   "metadata": {},
   "source": [
    "## Context Compression Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe122f",
   "metadata": {},
   "source": [
    "Here, we'll explore **LLM prompt-based context compression** strategies. The context compression can happen in the form of:\n",
    "\n",
    "- **Extractor**: Remove parts of the content of retrieved documents which are not relevant to the query. This is done by extracting only relevant parts of the document to the given query\n",
    "\n",
    "- **Filter**: Filter out documents which are not relevant to the given query but do not remove content from the document\n",
    "\n",
    "Good to also read about [Microsoft LLMLingua Prompt\n",
    "Compression](https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e32d3",
   "metadata": {},
   "source": [
    "### LLMChainExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec58a3",
   "metadata": {},
   "source": [
    "Here we look at `LLMChainExtractor`, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query. Totally irrelevant documents might also be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6591af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4760bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize retriever\n",
    "retriever = vector_store_chroma.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95ee960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts from each document only the content that is relevant to the query\n",
    "compressor = LLMChainExtractor.from_llm(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fafc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the documents similar to query and then applies the compressor\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e3dfc0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'start_index': 0, 'source': 'https://www.truefoundry.com'}, page_content='Orchestrate Agentic AI with AI Gateway\\nEnable intelligent multi-step reasoning, tool usage, and memory with full control and visibility across your AI agents and workflows.\\n \\nAI Gateway\\nManage agent memory, tool orchestration, and action planning through a centralized protocol that supports complex, context-aware workflows.\\nLearn More'),\n",
       " Document(metadata={'start_index': 790, 'source': 'https://www.truefoundry.com/ai-gateway'}, page_content='>>> Store and inspect full request/response logs centrally to ensure compliance and simplify debugging.\\n>>> Tag traffic with metadata like user ID, team, or environment to gain granular insights.\\n>>> Quota & Access Control via AI Gateway\\n>>> Enforce governance, control costs, and reduce risk with consistent policy management.\\n>>> Apply rate limits per user, service, or endpoint.\\n>>> Set cost-based or token-based quotas using metadata filters.\\n>>> Use role-based access control (RBAC) to isolate and manage usage.\\n>>> Ensuring predictable usage, strong access boundaries, and scalable team-level governance for your GenAI infrastructure.'),\n",
       " Document(metadata={'source': 'https://www.truefoundry.com/ai-gateway', 'start_index': 2351}, page_content='>>>\\nServe Self-Hosted Models Expose open-source models with full control.\\nDeploy LLaMA, Mistral, Falcon, and more with zero SDK changes.\\nFull compatibility with vLLM, SGLang, KServe, and Triton.\\nRun your own models in VPC, hybrid, or air-gapped environments.Read More\\n>>>\\nAI Gateway + MCP Integration\\nPower secure agent workflows through the AI Gateway’s native MCP support.\\nConnect enterprise tools like Slack, GitHub, Confluence, and Datadog.\\nEasily register internal MCP Servers with minimal setup required.\\n>>>')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = compression_retriever.invoke(question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a95b3a",
   "metadata": {},
   "source": [
    "### LLMChainFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b83ea",
   "metadata": {},
   "source": [
    "The `LLMChainFilter` is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6781ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b188fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize retriever\n",
    "retriever = vector_store_chroma.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3169daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decides which of the initially retrieved documents to filter out and which ones to return\n",
    "_filter = LLMChainFilter.from_llm(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cab61dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the documents similar to query and then applies the filter\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e364f5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='af769fe8-3114-4311-8ed6-ed3e10073295', metadata={'source': 'https://www.truefoundry.com', 'start_index': 0}, page_content='Orchestrate Agentic AI with AI GatewayEnable intelligent multi-step reasoning, tool usage, and memory with full control and visibility across your AI agents and workflows.AI GatewayManage agent memory, tool orchestration, and action planning through a centralized protocol that supports complex, context-aware workflows.Learn More\\n\\nMCP & Agents RegistryMaintain a structured, discoverable registry of tools and APIs accessible to agents, complete with schema validation and access control.Learn More\\n\\nPrompt Lifecycle ManagementVersion, manage, and monitor prompts to ensure high-quality, repeatable behavior across agents and use cases.Learn More'),\n",
       " Document(id='bf8ae97d-95b6-4488-b7c1-c01e56d1b477', metadata={'start_index': 790, 'source': 'https://www.truefoundry.com/ai-gateway'}, page_content='Store and inspect full request/response logs centrally to ensure compliance and simplify debugging.\\n\\nTag traffic with metadata like user ID, team, or environment to gain granular insights.\\n\\nFilter logs and metrics by model, team, or geography to quickly pinpoint root causes and accelerate resolution.Read MoreQuota & Access Control via AI GatewayEnforce governance, control costs, and reduce risk with consistent policy management.\\n\\nApply rate limits per user, service, or endpoint.\\n\\nSet cost-based or token-based quotas using metadata filters.\\n\\nUse role-based access control (RBAC) to isolate and manage usage.\\n\\nGovern service accounts and agent workloads at scale through centralized rules.Read MoreEnsuring predictable usage, strong access boundaries, and scalable team-level governance for your GenAI infrastructure.Low-Latency Inference\\xa0Run your most performance-sensitive workloads through a high-speed infrastructure.\\n\\nAchieve sub-3ms internal latency even under enterprise-scale workloads.'),\n",
       " Document(id='e0595d54-3eda-4bcd-98a2-9a4dafc26d49', metadata={'source': 'https://www.truefoundry.com/ai-gateway', 'start_index': 2351}, page_content='Distribute traffic intelligently using weighted load balancing for reliability and scale.\\n\\nAutomatically fallback to secondary models when a request fails.\\n\\nUse geo-aware routing to meet regional compliance and availability needs.Read MoreThis system ensures you never go offline, even when individual models face downtime or spike in latency.Serve Self-Hosted Models Expose open-source models with full control.\\n\\nDeploy LLaMA, Mistral, Falcon, and more with zero SDK changes.\\n\\nFull compatibility with vLLM, SGLang, KServe, and Triton.\\n\\nStreamline operations with Helm-based management of autoscaling, GPU scheduling, and deployments\\n\\nRun your own models in VPC, hybrid, or air-gapped environments.Read MoreAI Gateway + MCP IntegrationPower secure agent workflows through the AI Gateway’s native MCP support.\\n\\nConnect enterprise tools like Slack, GitHub, Confluence, and Datadog.\\n\\nEasily register internal MCP Servers with minimal setup required.')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = compression_retriever.invoke(question)\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
