{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1932c10-da98-4ca5-a0b2-9bebea592a49",
   "metadata": {},
   "source": [
    "# Building LLM Applications with Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3aea71-8c82-4b5a-8561-bf5c05023767",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27192e35-2477-4a8a-ab77-f2c81a8f6a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, re, getpass, warnings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32293576-26c8-430e-89cc-4c86dc0bda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Groq API Key\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"GROQ API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab54c83-b140-452a-b386-1b15f8fceae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting chat model\n",
    "model_name = \"llama-3.1-8b-instant\"\n",
    "llm = init_chat_model(model_name, model_provider=\"groq\", configurable_fields=(\"temperature\",\"max_tokens\")) #Other Llama alternatives available are llama3-8b-8192, llama-3.3-70b-versatile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f1564-ea3b-4ab6-9968-caa79c6ce561",
   "metadata": {},
   "source": [
    "### Getting Started with Gradio `gr.Interface` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5dd104e-6032-437e-ac12-92611a90a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def get_llm_response(input):\n",
    "    output = llm.invoke(input).content\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f79e2-8cee-41b9-b07e-734659258009",
   "metadata": {},
   "source": [
    "#### Basic App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c066a-f036-412e-a29d-397eaec7cd2a",
   "metadata": {},
   "source": [
    "Interface is Gradio's main high-level class, and allows you to create a web-based GUI / demo around a machine learning model (or any Python function) in a few lines of code. You must specify three parameters: (1) the function to create a GUI for (2) the desired input components and (3) the desired output components. Additional parameters can be used to control the appearance and behavior of the demo. See more [here](https://www.gradio.app/docs/gradio/interface). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5790369c-ce6a-45b6-acbe-74fcd209b4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7886\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7886/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.close_all()\n",
    "demo = gr.Interface(fn=get_llm_response, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch() #can also mention server_port as an argument (7860,7861 etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dac3c9-fd96-4e0f-ad3c-75a56874c205",
   "metadata": {},
   "source": [
    "`demo.launch(share=True)` lets you create a public link to share with your team or friends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced9cde-620a-4a3d-9e44-8f103a74124f",
   "metadata": {},
   "source": [
    "#### Customizing the Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0a8c3-05dd-4165-aac8-5a9f50bfc983",
   "metadata": {},
   "source": [
    "- Notice below that we pass in a list `[]` to `inputs` and to `outputs` because the function `fn` (in this case, `summarize()`, can take in more than one input and return more than one output.\n",
    "- The number of objects passed to `inputs` list should match the number of parameters that the `fn` function takes in, and the number of objects passed to the `outputs` list should match the number of objects returned by the `fn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630391c0-fb9f-466a-97e6-8097d54ac019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7886\n",
      "* Running on local URL:  http://127.0.0.1:7886\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7886/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.close_all()\n",
    "demo = gr.Interface(fn=get_llm_response, \n",
    "                    inputs=[gr.Textbox(label=\"Input Prompt\", lines=6)],\n",
    "                    outputs=[gr.Textbox(label=\"Output by LLM\", lines=3)],\n",
    "                    title=\"My Personal Chatbot\",\n",
    "                    description=\"Chat on anything using the Llama3.1 model under the hood!\"\n",
    "                   )\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6e63f-506f-4a28-b5d4-64cf9ddd2c58",
   "metadata": {},
   "source": [
    "### Building Gradio App using Gradio `gr.Interface` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea50a2c3-f4aa-4c4d-a688-f5143b83a3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "def generate(input, max_tokens):\n",
    "    output = llm.with_config({\"max_tokens\": max_tokens}).invoke(input).content\n",
    "    return output\n",
    "\n",
    "demo = gr.Interface(fn=generate, \n",
    "                    inputs=[gr.Textbox(label=\"Prompt\"), \n",
    "                            gr.Slider(label=\"Max New Tokens\", \n",
    "                                      value=500,  \n",
    "                                      maximum=1000, \n",
    "                                      minimum=10)], \n",
    "                    outputs=[gr.Textbox(label=\"Completion\")])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de2a94-83b6-4ed0-a029-c890746f41b0",
   "metadata": {},
   "source": [
    "### Getting Started with Gradio `gr.Chatbot` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8383a17-42df-4fcf-ab51-9eb81f4ffbdf",
   "metadata": {},
   "source": [
    "Creates a chatbot that displays user-submitted messages and responses. Supports a subset of Markdown including bold, italics, code, tables. Also supports audio/video/image files, which are displayed in the Chatbot, and other kinds of files which are displayed as links. This component is usually used as an output component. See more [here](https://www.gradio.app/docs/gradio/chatbot)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09aef4d-8a87-4427-81f5-6e80f25af2b3",
   "metadata": {},
   "source": [
    "- `gr.Chatbot()` allows you to save the chat history (between the user and the LLM) as well as display the dialogue in the app.\n",
    "- Define your `fn` to take in a `gr.Chatbot()` object.  \n",
    "  - Within your defined `fn` function, extend with a list containing the user message and the LLM's response in OpenAI's messages format that's widely used:\n",
    "`chatbot_object.append( [{'role':'user','content':user_message}, {'role':'assistant','content':llm_message}] )`\n",
    "\n",
    "- Include the chatbot object in both the inputs and the outputs of the app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310fc869-c70e-457b-897d-51cafafb6d4f",
   "metadata": {},
   "source": [
    "#### Basic App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf053394-8968-447c-aaef-dfd4d775e33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        #No LLM here, just respond with a random pre-made message\n",
    "        bot_message = random.choice([\"Tell me more about it\", \n",
    "                                     \"Cool, but I'm not interested\", \n",
    "                                     \"Hmmmm, ok then\"]) \n",
    "        chat_history.extend([{'role':'user','content': message},\n",
    "                            {'role':'assistant','content': bot_message}])\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240, type='messages') #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66fa8d5-8330-47d3-93e5-58821c82299e",
   "metadata": {},
   "source": [
    "#### Format the prompt with the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89292b7d-1e7e-4df0-8e98-3784a1e305bd",
   "metadata": {},
   "source": [
    "- You can iterate through the chatbot object with a for loop.\n",
    "- Each item is a tuple containing the user message and the LLM's message.\n",
    "\n",
    "```Python\n",
    "for turn in chat_history:\n",
    "    user_msg, bot_msg = turn\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b22caa5-5322-4f8d-873e-a37f9b8fe672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(message, chat_history):\n",
    "    prompt = \"\"\n",
    "    for i in range(len(chat_history) - 1):\n",
    "    # Ensure that we are processing pairs and not going out of bounds\n",
    "        if i % 2 == 0: # Only process if i is an even number (start of a pair)\n",
    "            user_message = chat_history[i][\"content\"]\n",
    "            bot_message = chat_history[i+1][\"content\"]\n",
    "            prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1bf64e09-b997-4c33-bfc6-3f7ed052ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(message, chat_history):\n",
    "        formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "        bot_message = llm.with_config({\"max_tokens\": 1024}).invoke(formatted_prompt).content\n",
    "        chat_history.extend([{'role':'user','content': message},\n",
    "                            {'role':'assistant','content': bot_message}])\n",
    "        return \"\", chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10cd8615-abae-40c9-926f-1a0f0d5732bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240, type='messages') #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5adc9aa7-393e-47bb-866f-97434d5b3107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.close_all()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21642940-96af-4eef-a587-d3460ccc9a16",
   "metadata": {},
   "source": [
    "#### Advanced Feature - System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6cb6a1f0-b009-4147-be8f-9fc763a6cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(message, chat_history, instruction):\n",
    "    prompt = f\"System:{instruction}\"\n",
    "    for i in range(len(chat_history) - 1):\n",
    "    # Ensure that we are processing pairs and not going out of bounds\n",
    "        if i % 2 == 0: # Only process if i is an even number (start of a pair)\n",
    "            user_message = chat_history[i][\"content\"]\n",
    "            bot_message = chat_history[i+1][\"content\"]\n",
    "            prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df419ec3-5a8d-4ebd-8145-133ab48d2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(message, chat_history):\n",
    "        instruction = \"You are a rockstar! Respond accordingly.\"\n",
    "        formatted_prompt = format_chat_prompt(message, chat_history, instruction)\n",
    "        bot_message = llm.with_config({\"max_tokens\": 1024}).invoke(formatted_prompt).content\n",
    "        chat_history.extend([{'role':'user','content': message},\n",
    "                            {'role':'assistant','content': bot_message}])\n",
    "        return \"\", chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf9d18bc-7c9d-4464-831f-b4d0409ebaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240, type='messages') #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "90d32520-b36a-4e77-984d-3454e75253f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Other functions remain the same!\n",
    "gr.close_all()\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
