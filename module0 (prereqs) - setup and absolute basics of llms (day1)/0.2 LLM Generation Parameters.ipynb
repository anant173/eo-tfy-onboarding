{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Generation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this! - What does this do?\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Chat Completions API (most widely used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OpenAI` Official documentation is here - https://platform.openai.com/docs/api-reference/chat/create Refer to this for a full set of parameters that we'll later explore. \n",
    "\n",
    "The most used parameters are as listed below, with definitions from official `OpenAI` API documentation. These are considered \"common\" because they fundamentally control how an autoregressive language model samples tokens to generate output. \n",
    "\n",
    "- **max_tokens** (only *max_completion_tokens* is supported by o1 series; no default): The maximum number of completion tokens that may be used over the course of the run. The run will make a best effort to use only the number of completion tokens specified, across multiple turns of the run. If the run exceeds the number of completion tokens specified, the run will end with status `incomplete`.\n",
    "- **temperature** (*1 by default*): What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.\n",
    "- **top_p** (*1 by default*): An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "We generally recommend altering this or temperature but not both.\n",
    "- **top_k**: not available in OpenAI client library\n",
    "- **frequency_penalty** (*0 by default*): Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
    "- **presence_penalty** (*0 by default*): Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "- **stop** (*null by default*): Not supported with latest reasoning models o3 and o4-mini. Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Anthropic's Messages API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to note that Anthropic's `Messages` API contains similar parameters except a few differences:\n",
    "\n",
    "- **max_tokens** is used as is\n",
    "- **temperature** has a value between 0 and 1\n",
    "- **top_p** is as is\n",
    "- **top_k** is also available: Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\n",
    "- **frequency_penalty** and **presence_penalty** parameters are not available.\n",
    "- **stop_sequences** is used instead of **stop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Some important considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Exact naming conventions of parameters can vary, but functionality is same (like we saw with Anthropic's API).\n",
    "- The optimal range or impact of a parameter might differ between models, even if general principle is same. A temperature of 1.0 might be very wild for one model but only moderately creative for another.\n",
    "- Differences between different parameter configurations will be more prominent in larger LLMs, as with smaller LLMs, the \"spread\" of probabilities for the next token might not be as wide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Setting temperature vs. top_p for different use cases: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "| Use Case               | Temperature | Top_p | Description                                                                                        |\n",
    "| :--------------------- | :---------- | :---- | :------------------------------------------------------------------------------------------------- |\n",
    "| Code Generation        | 0.2         | 0.1   | Generates code that adheres to established patterns and conventions. Output is more deterministic and focused. Useful for generating syntactically correct code. |\n",
    "| Creative Writing       | 0.7         | 0.8   | Generates creative and diverse text for storytelling. Output is more exploratory and less constrained by patterns. |\n",
    "| Chatbot Responses      | 0.5         | 0.5   | Generates conversational responses that balance coherence and diversity. Output is more natural and engaging. |\n",
    "| Code Comment Generation | 0.3         | 0.2   | Generates code comments that are more likely to be concise and relevant. Output is more deterministic and adheres to conventions. |\n",
    "| Data Analysis Scripting | 0.2         | 0.1   | Generates data analysis scripts that are more likely to be correct and efficient. Output is more deterministic and focused. |\n",
    "| Exploratory Code Writing | 0.6         | 0.7   | Generates code that explores alternative solutions and creative approaches. Output is less constrained by established patterns. |\n",
    "\n",
    "References from here: https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api/172683/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Use either Llama3.2 (3B) with Ollama\n",
    "!ollama pull llama3.2    \n",
    "\n",
    "# Initialize the OpenAI client to connect to Ollama\n",
    "llm_api = OpenAI(base_url='http://localhost:11434/v1', \n",
    "                 api_key='ollama') # note that llm_api is a good variable name, because the LLM is indeed exposed locally as an API by Ollama!\n",
    "\n",
    "# Define the model to use\n",
    "model_name = \"llama3.2\" # Ensure this matches the pulled model in Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Or use DeepSeek-R1-Distill-Llama-70B from Groq\n",
    "# llm_api = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "\n",
    "# # Define the model to use\n",
    "# model_name = \"deepseek-r1-distill-llama-70b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for text generation\n",
    "def generate_and_display(prompt, param_name, param_value, num_runs=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to make a call to the LLM and display results.\n",
    "    Runs multiple times for stochastic parameters to highlight differences.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Demonstrating {param_name}={param_value} ---\")\n",
    "    print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "    print(f\"Other consistent parameters: {kwargs}\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful and concise AI assistant for a business.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        print(f\"\\n--- Run {i+1} ---\")\n",
    "        try:\n",
    "            response = llm_api.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                **{param_name: param_value, **kwargs}\n",
    "            )\n",
    "            print(\"Generated Text:\")\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            display(Markdown(answer)) # .strip() removes leading/trailing whitespace for cleaner output\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Ensure Ollama is running and the model is pulled. Skipping further runs for this parameter setting.\")\n",
    "            break # Exit loop if an error occurs\n",
    "\n",
    "    print(\"\\n--- End Demonstration for this parameter setting ---\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Content generation (marketing copy, ad variations, blog post ideas), creative brainstorming for product features, unique customer engagement messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to expect:**\n",
    "- **Low Temperature**: Look for very similar, factual, and direct outputs across runs. Less \"fluff,\" more to the point.\n",
    "    - E.g., Consistent, factual product bullet points for a catalog\n",
    "- **Medium Temperature**: Outputs will start showing more natural language, some variation in phrasing, but still coherent.\n",
    "    - E.g., Engaging marketing copy for a landing page\n",
    "- **High Temperature**: Expect more varied vocabulary, potentially more metaphorical or unusual phrasing, and noticeable differences between runs. It might sometimes deviate slightly from the core prompt if pushed too high.\n",
    "    - E.g., Brainstorming unique ad slogans or creative concepts for a campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_temp = \"\"\"\n",
    "Reliance is heavily investing in Generative AI to serve India's 'next billion users'. \n",
    "Envision a new GenAI-powered service that leverages Reliance's massive user base and diverse data. \n",
    "First think about which sector you want to pick for this GenAI-powered service, and it can be one of: Oil to Chemicals, Digital Services, \n",
    "Retail, New Energy & Materials, Media & Entertainment, Oil & Gas Exploration & Production, Financial Services, Textiles, Infrastructure.\n",
    "Then, describe this service, focusing on its innovative features and how it will enhance daily life for a typical Indian user.\n",
    "Keep the answer less than 70 tokens.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating temperature=0.1 ---\n",
      "Prompt: \"\n",
      "Reliance is heavily investing in Generative AI to serve India's 'next billion users'. \n",
      "Envision a new GenAI-powered service that leverages Reliance's massive user base and diverse data. \n",
      "First think about which sector you want to pick for this GenAI-powered service, and it can be one of: Oil to Chemicals, Digital Services, \n",
      "Retail, New Energy & Materials, Media & Entertainment, Oil & Gas Exploration & Production, Financial Services, Textiles, Infrastructure.\n",
      "Then, describe this service, focusing on its innovative features and how it will enhance daily life for a typical Indian user.\n",
      "Keep the answer less than 70 tokens.\"\n",
      "Other consistent parameters: {'max_completion_tokens': 70}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I propose a GenAI-powered service in the Retail sector: \"Jagat\" - a personalized e-commerce platform.\n",
       "\n",
       "Jagat leverages Reliance's diverse data to offer AI-driven product recommendations, inventory management, and supply chain optimization. \n",
       "\n",
       "Key features:\n",
       "\n",
       "* AI-powered shopping assistant: provides users with tailored suggestions based on their purchase history and preferences.\n",
       "* Virtual try-on: uses AR/VR technology for seamless virtual product demonstrations.\n",
       "* Smart home delivery: integrates with customers' smart home devices for efficient package tracking and delivery.\n",
       "\n",
       "Jagat aims to enhance daily life by making online shopping more convenient, personalized, and enjoyable for Indian users."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I propose a GenAI-powered service in the Retail sector: \"Jagat\" - an AI-driven personalized shopping assistant.\n",
       "\n",
       "Jagat uses machine learning to analyze users' purchase history, preferences, and lifestyle to offer tailored product recommendations, exclusive deals, and personalized services. The service integrates with Reliance's vast network of retail stores, e-commerce platforms, and logistics.\n",
       "\n",
       "Innovative features include:\n",
       "\n",
       "* AI-powered styling suggestions for clothing and home decor\n",
       "* Personalized meal planning and grocery delivery\n",
       "* Virtual try-on for fashion items\n",
       "* Real-time price tracking and alerts\n",
       "\n",
       "Jagat enhances daily life by making shopping more convenient, efficient, and enjoyable. By leveraging Reliance's massive user base and diverse data, Jagat aims to become the go-to retail assistant for India's 'next billion users'."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I propose a GenAI-powered service in the Retail sector: \"Jagat\" - a personalized shopping assistant.\n",
       "\n",
       "Jagat leverages Reliance's diverse data to offer users:\n",
       "\n",
       "* AI-driven product recommendations based on purchase history and preferences\n",
       "* Virtual try-on for clothing and accessories\n",
       "* Augmented reality (AR) shopping experiences\n",
       "* Personalized customer service using chatbots and voice assistants\n",
       "\n",
       "Jagat aims to enhance daily life by making shopping more convenient, efficient, and enjoyable. By harnessing the power of GenAI, Jagat can provide users with a seamless and personalized shopping experience, revolutionizing the way Indians shop and interact with retail services."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 1.1: Low Temperature\n",
    "generate_and_display(\n",
    "    common_prompt_temp,\n",
    "    \"temperature\", 0.1,\n",
    "    num_runs=3, # Run multiple times to show consistency\n",
    "    max_completion_tokens=70\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating temperature=0.7 ---\n",
      "Prompt: \"\n",
      "Reliance is heavily investing in Generative AI to serve India's 'next billion users'. \n",
      "Envision a new GenAI-powered service that leverages Reliance's massive user base and diverse data. \n",
      "First think about which sector you want to pick for this GenAI-powered service, and it can be one of: Oil to Chemicals, Digital Services, \n",
      "Retail, New Energy & Materials, Media & Entertainment, Oil & Gas Exploration & Production, Financial Services, Textiles, Infrastructure.\n",
      "Then, describe this service, focusing on its innovative features and how it will enhance daily life for a typical Indian user.\n",
      "Keep the answer less than 70 tokens.\"\n",
      "Other consistent parameters: {'max_completion_tokens': 70}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I propose a GenAI-powered service in the Retail sector: \"DreamMart\".\n",
       "\n",
       "DreamMart leverages Reliance's vast data and user base to create a personalized, AI-driven shopping experience. Users receive customized recommendations based on their preferences, purchase history, and lifestyle.\n",
       "\n",
       "Innovative features include:\n",
       "\n",
       "* Virtual try-on for clothing and accessories\n",
       "* AI-powered product reviews and ratings\n",
       "* Personalized offers and discounts\n",
       "* Smart inventory management for in-store experiences\n",
       "\n",
       "By enhancing daily life with DreamMart, typical Indian users can enjoy a seamless shopping experience, discover new products, and make informed purchasing decisions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I propose a GenAI-powered \"EcoLife\" service in the Retail sector.\n",
       "\n",
       "**EcoLife**:\n",
       "\n",
       "* A personal shopping assistant using AI-driven recommendations, suggesting eco-friendly products based on users' preferences and daily habits.\n",
       "* Integrates with Reliance's vast data repository to analyze user behavior, offer personalized discounts, and provide real-time suggestions for sustainable living.\n",
       "* Features:\n",
       "\t+ \"Green Profile\": Users create a digital profile showcasing their environmental footprint, helping them track progress towards sustainability goals.\n",
       "\t+ \"Eco-Alerts\": AI-powered notifications alerting users about sales on eco-friendly products, new arrivals, or special promotions.\n",
       "\n",
       "By leveraging Reliance's massive user base and diverse data, EcoLife aims to make sustainable living more accessible, convenient, and affordable for the Indian masses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I propose Reliance's GenAI-powered service in the Retail sector: \"Jagruk Mart\" (Aware Market).\n",
       "\n",
       "**Service Description:** Jagruk Mart uses GenAI to create a personalized, voice-activated shopping assistant that helps users discover and purchase products based on their preferences, location, and needs.\n",
       "\n",
       "**Innovative Features:**\n",
       "\n",
       "* AI-driven product recommendations\n",
       "* Real-time inventory updates\n",
       "* Personalized offers and discounts\n",
       "* Integration with e-wallets and mobile payments\n",
       "\n",
       "**Enhancing Daily Life:** Jagruk Mart enhances daily life by streamlining shopping experiences, saving time, and providing users with relevant product suggestions. It also fosters a more inclusive retail ecosystem, catering to the diverse needs of India's \"next billion users.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 1.2: Medium Temperature\n",
    "generate_and_display(\n",
    "    common_prompt_temp,\n",
    "    \"temperature\", 0.7,\n",
    "    num_runs=3, # Run multiple times to show controlled variation\n",
    "    max_completion_tokens=70\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating temperature=1.5 ---\n",
      "Prompt: \"\n",
      "Reliance is heavily investing in Generative AI to serve India's 'next billion users'. \n",
      "Envision a new GenAI-powered service that leverages Reliance's massive user base and diverse data. \n",
      "First think about which sector you want to pick for this GenAI-powered service, and it can be one of: Oil to Chemicals, Digital Services, \n",
      "Retail, New Energy & Materials, Media & Entertainment, Oil & Gas Exploration & Production, Financial Services, Textiles, Infrastructure.\n",
      "Then, describe this service, focusing on its innovative features and how it will enhance daily life for a typical Indian user.\n",
      "Keep the answer less than 70 tokens.\"\n",
      "Other consistent parameters: {'max_completion_tokens': 70}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sector: Digital_services\n",
       "\n",
       "Reliance's GenAI-powered service: \"SmartLife Partner\"\n",
       "\n",
       "This AI-driven platform leverages Reliance's vast data assets to provide personalized lifestyle solutions, bridging the digital divide for India's 'next billion users.' The service seamlessly integrates with smartphones to:\n",
       "\n",
       "1. Auto-detect and assist in health monitoringsolutions using computer vision analytics.\n",
       "2. Recognize local accents using voice-assist feature & respond instantly in more than 10 langauges.\n",
       "3. Predictively offer travel suggestions, based on real-time traffic patterns.\n",
       "\n",
       "Smartlife Partner uses artificial learning networks that help identify the exact need & needs and suggest options using this advanced information analysis with every passing day.\n",
       "\n",
       "With ease & access from anywhere, 'life can get easy & smarter in a flash'."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sector: Digital Services\n",
       "\n",
       "Service Name: ReliaMentor\n",
       "\n",
       "ReliaMentor combines AI-assisted learning platforms with personalized skill development. \n",
       "\n",
       "- **Personalized Career Guidance**: AI-drives an extensive network assessment to provide guidance for students/learners choosing industries for themselves.\n",
       "  \n",
       "This service empowers daily users by helping them discover apt career choices using advanced, large dataset-driven assessments powered by ReLiannce's rich content and comprehensive expertise from over a Billion users in digitl spaces - including its popular educational programs (Jio TV & Digital Movies) ,JioMeet  and Quizzes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's focus on Education.\n",
       "\n",
       "**GenAI-powered \"DreamCareer\" - A Virtual Career Counsellor**\n",
       "\n",
       "For Reliance: Developing advanced AI technology to create an immersive, AI-assisted career guidance platform, integrating machine learning insights, mentorship, and resources for Indians aspiring to reach new goals. In-app tools offer guidance on resume building, interviewing strategy, skill development course recommendations, online learning platforms, networking opportunities and employability enhancement training! The 'DreamCareer' Gen AI service brings career goals within reach of millions, redefining India's future workforce."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 1.3: High Temperature\n",
    "generate_and_display(\n",
    "    common_prompt_temp,\n",
    "    \"temperature\", 1.5, # Pushing temperature higher for more dramatic effect\n",
    "    num_runs=3, # Run multiple times to show increased randomness\n",
    "    max_completion_tokens=70\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Chatbot response length control, email subject line generation, tweet generation, summarizing lengthy documents for internal reports, generating specific-length ad copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to expect**: \n",
    "\n",
    "- The generated output will strictly adhere to the `max_tokens` limit, regardless of whether the summary is complete.\n",
    "- Depending on the use case, the `max_tokens` parameter can be set:\n",
    "    - Very few tokens (e.g., for a headline or quick alert)\n",
    "    - Moderate tokens (e.g., for an executive summary or internal Slack message)\n",
    "    - More tokens (e.g., for a detailed summary section in a newsletter or report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_max_tokens = \"\"\"\n",
    "Imagine you're a data scientist working with an IPL team. \n",
    "Describe how Generative AI could be used to create novel training drills or strategic simulations for batsmen, considering the unique challenges of Indian pitches and varied bowling styles.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating max_tokens=15 ---\n",
      "Prompt: \"\n",
      "Imagine you're a data scientist working with an IPL team. \n",
      "Describe how Generative AI could be used to create novel training drills or strategic simulations for batsmen, considering the unique challenges of Indian pitches and varied bowling styles. Complete response in less than 15 tokens.\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As a data scientist, I'd leverage Generative AI to generate:\n",
       "\n",
       "1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 2.1: Very few tokens\n",
    "generate_and_display(\n",
    "    common_prompt_max_tokens + \" Complete response in less than 15 tokens.\",\n",
    "    \"max_tokens\", 15,\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating max_tokens=50 ---\n",
      "Prompt: \"\n",
      "Imagine you're a data scientist working with an IPL team. \n",
      "Describe how Generative AI could be used to create novel training drills or strategic simulations for batsmen, considering the unique challenges of Indian pitches and varied bowling styles.\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As a data scientist working with an IPL team, I'd like to explore the potential of Generative AI in creating novel training drills and strategic simulations for batsmen. Here's how:\n",
       "\n",
       "**Novel Training Drills:**\n",
       "\n",
       "1. **Simulated"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 2.2: Moderate tokens\n",
    "generate_and_display(\n",
    "    common_prompt_max_tokens,\n",
    "    \"max_tokens\", 50,\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating max_tokens=150 ---\n",
      "Prompt: \"\n",
      "Imagine you're a data scientist working with an IPL team. \n",
      "Describe how Generative AI could be used to create novel training drills or strategic simulations for batsmen, considering the unique challenges of Indian pitches and varied bowling styles.\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As a data scientist working with an IPL team, I'd love to explore the potential of Generative AI in creating innovative training drills and strategic simulations for batsmen. Here's how:\n",
       "\n",
       "**Generative AI Applications**\n",
       "\n",
       "1. **Batting Drills**: Utilize Generative AI to create personalized batting drills based on individual batsmen's strengths, weaknesses, and playing styles. The algorithm can generate a variety of scenarios, such as:\n",
       "\t* Mimicking specific bowlers' deliveries (e.g., Jofra Archer's bouncers or Jasprit Bumrah's yorkers).\n",
       "\t* Simulating different pitch conditions (e.g., the Gargantuan turner at Eden Gardens or the seam-friendly surfaces at Motera Stadium)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 2.3: More tokens\n",
    "generate_and_display(\n",
    "    common_prompt_max_tokens,\n",
    "    \"max_tokens\", 150,\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating max_tokens=1000 ---\n",
      "Prompt: \"\n",
      "Imagine you're a data scientist working with an IPL team. \n",
      "Describe how Generative AI could be used to create novel training drills or strategic simulations for batsmen, considering the unique challenges of Indian pitches and varied bowling styles.\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As a data scientist working with an IPL team, I'd love to explore the potential of Generative AI in creating novel training drills and strategic simulations for batsmen. Here's how:\n",
       "\n",
       "**Data Collection**\n",
       "\n",
       "To create effective generative models, we'll need access to vast amounts of high-quality data on Indian pitches, bowling styles, and batting performances. This can include:\n",
       "\n",
       "1. Match footage and analysis from various IPL matches.\n",
       "2. Pitch reports and weather conditions.\n",
       "3. Bowling action data (e.g., spin, pace, trajectory).\n",
       "4. Batting performance metrics (e.g., strike rate, average, boundaries).\n",
       "\n",
       "**Generative AI Models**\n",
       "\n",
       "We'll employ Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs) to generate novel training drills and strategic simulations for batsmen.\n",
       "\n",
       "1. **Pitch Generator**: Train a GAN or VAE to generate new pitch scenarios based on historical data, taking into account factors like spin, pace, and weather conditions.\n",
       "2. **Bowling Style Simulator**: Develop a model that can simulate different bowling styles (e.g., leg-spin, off-spin, fast bowling) with varying degrees of difficulty.\n",
       "3. **Batting Performance Predictor**: Train a VAE to predict batting performance metrics (e.g., strike rate, average, boundaries) based on pitch and bowling style inputs.\n",
       "\n",
       "**Training Drill Generator**\n",
       "\n",
       "Using the generated pitch scenarios and bowling style simulations, we'll create novel training drills for batsmen:\n",
       "\n",
       "1. **Adaptive Training Drills**: Generate training drills that adapt to the batsman's current form and strengths.\n",
       "2. **Scenario-Based Training**: Create simulations of specific match scenarios (e.g., chasing down a target, facing pressure situations).\n",
       "3. **Personalized Training Plans**: Develop customized training plans for each batsman based on their performance metrics and preferences.\n",
       "\n",
       "**Strategic Simulation**\n",
       "\n",
       "To aid the team's strategic decisions during matches:\n",
       "\n",
       "1. **Real-Time Pitch Analysis**: Use GANs or VAEs to generate real-time pitch analysis, providing insights into the current conditions.\n",
       "2. **Bowling Style Prediction**: Develop a model that predicts the likelihood of different bowling styles being used in the match.\n",
       "3. **Batting Strategy Optimization**: Train a machine learning model to optimize batting strategies based on historical data and real-time inputs.\n",
       "\n",
       "**Implementation**\n",
       "\n",
       "To integrate these generative models into our training processes:\n",
       "\n",
       "1. **Data Integration**: Integrate generated data with existing datasets to create a comprehensive knowledge graph of Indian pitches, bowling styles, and batting performances.\n",
       "2. **Training Software**: Develop custom software to visualize and interact with the generative models, enabling coaches and trainers to easily explore and adapt the simulations.\n",
       "3. **Real-Time Feedback Loops**: Implement real-time feedback loops to ensure that batsmen receive immediate, actionable insights from the generative models.\n",
       "\n",
       "By leveraging Generative AI in this way, we can create novel training drills and strategic simulations that cater to the unique challenges of Indian pitches and varied bowling styles, ultimately enhancing our team's performance and competitiveness."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Example 2.4: Even more max_tokens\n",
    "generate_and_display(\n",
    "    common_prompt_max_tokens,\n",
    "    \"max_tokens\", 1000,\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Top_p (Nucleus sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Generating diverse customer service responses (without going off-topic), creating varied product names, generating slightly different content variations for A/B testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to Observe**:\n",
    "\n",
    "- **Low** top_p: Outputs will be very similar across runs, often picking the most statistically common words/phrases. Less variation in phrasing.\n",
    "\n",
    "- **Medium** top_p: Some variations in phrasing will emerge, but still within a coherent and expected range.\n",
    "\n",
    "- **High** top_p: More diverse and potentially more \"creative\" word choices and phrasing, showing greater variation across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_top_p = \"\"\"\n",
    "A customer is asking about our new premium coffee blend, that rivals Starbucks, Peets Coffee and Cafe Coffee Day. \n",
    "Provide a welcoming response that also highlights its unique flavor notes.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating top_p=0.1 ---\n",
      "Prompt: \"\n",
      "A customer is asking about our new premium coffee blend, that rivals Starbucks, Peets Coffee and Cafe Coffee Day. \n",
      "Provide a welcoming response that also highlights its unique flavor notes.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 80}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Welcome to [Your Business Name]! We're thrilled you're interested in our new premium coffee blend. I'd be happy to tell you more about it.\n",
       "\n",
       "Our signature blend is carefully crafted to rival the best of the industry, and we're confident you'll love it just as much as our loyal customers do. What sets us apart is its unique flavor profile, which combines notes of rich chocolate"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Welcome to [Your Business Name]! We're thrilled you're interested in our new premium coffee blend. I'd be happy to tell you more about it.\n",
       "\n",
       "Our expertly crafted blend is indeed a game-changer, and we're confident it will satisfy even the most discerning palates. With notes of rich chocolate, velvety smooth caramel, and hints of exotic spices like cardam"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Welcome to [Your Business Name]! We're thrilled you're interested in our new premium coffee blend. I'd be happy to tell you more about it.\n",
       "\n",
       "Our expertly crafted blend is indeed a game-changer, and we're confident it will satisfy even the most discerning palates. With notes of rich chocolate, velvety smooth caramel, and hints of exotic spices like cardam"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 3.1: Low top_p (for a very standard, consistent customer service greeting)\n",
    "generate_and_display(\n",
    "    common_prompt_top_p,\n",
    "    \"top_p\", 0.1,\n",
    "    num_runs=3,\n",
    "    temperature=0.7,\n",
    "    max_tokens=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating top_p=0.5 ---\n",
      "Prompt: \"\n",
      "A customer is asking about our new premium coffee blend, that rivals Starbucks, Peets Coffee and Cafe Coffee Day. \n",
      "Provide a welcoming response that also highlights its unique flavor notes.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 80}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a warm and inviting response to the customer's inquiry:\n",
       "\n",
       "\"Thank you for your interest in our new premium coffee blend! We're thrilled to introduce our latest creation, carefully crafted to rival the best of the industry - Starbucks, Peet's Coffee, and Cafe Coffee Day.\n",
       "\n",
       "Our expert roasters have worked tirelessly to bring you a truly exceptional cup, with a rich and smooth flavor profile that"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a warm and inviting response to the customer's inquiry:\n",
       "\n",
       "\"Thank you for your interest in our premium coffee blend! We're thrilled to introduce our newest addition, carefully crafted to rival the best of the industry - Starbucks, Peets Coffee, and Cafe Coffee Day.\n",
       "\n",
       "Our expert roasters have meticulously blended beans from around the world to create a truly unique flavor profile. Our signature blend is characterized"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a warm and inviting response to the customer's inquiry:\n",
       "\n",
       "\"Thank you for reaching out about our new premium coffee blend! We're thrilled to introduce you to our latest creation, carefully crafted to rival the best of the industry. Our expert roasters have carefully selected the finest Arabica beans from around the world to create a truly unique flavor profile.\n",
       "\n",
       "What sets our blend apart is its bold and"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 3.2: Medium top_p (for a friendly, slightly varied customer service interaction)\n",
    "generate_and_display(\n",
    "    common_prompt_top_p,\n",
    "    \"top_p\", 0.5,\n",
    "    num_runs=3,\n",
    "    temperature=0.7,\n",
    "    max_tokens=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating top_p=0.99 ---\n",
      "Prompt: \"\n",
      "A customer is asking about our new premium coffee blend, that rivals Starbucks, Peets Coffee and Cafe Coffee Day. \n",
      "Provide a welcoming response that also highlights its unique flavor notes.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 80}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Welcome to [Your Business Name]! We're thrilled you're interested in our new premium coffee blend. I'd be happy to tell you more about it.\n",
       "\n",
       "Our 'Cafe Royale' blend is indeed a game-changer, and many of our customers have already fallen in love with its rich, smooth flavor profile. What sets it apart from the big names like Starbucks, Peet's Coffee"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Thank you for stopping by! I'd be delighted to tell you more about our new premium coffee blend. We're thrilled to introduce 'Elevé,' carefully crafted to rival the best in the industry - Starbucks, Peet's Coffee, and Cafe Coffee Day.\n",
       "\n",
       "What sets our Elevé blend apart is its unique flavor profile, which combines rich notes of dark chocolate, hints of smoky caramel"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Welcome to our cafe! We're thrilled to introduce you to our newest premium coffee blend, carefully crafted to rival the best of the industry. Our expert roasters have worked tirelessly to create a truly unique and exceptional taste experience.\n",
       "\n",
       "Our signature blend is a masterful fusion of South American and East African beans, showcasing rich notes of dark chocolate, hints of caramel, and subtle undertones of citrus. But"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 3.3: High top_p (for brainstorming diverse opening lines for marketing outreach)\n",
    "generate_and_display(\n",
    "    common_prompt_top_p,\n",
    "    \"top_p\", 0.99, # Pushing higher for more noticeable effect\n",
    "    num_runs=3,\n",
    "    temperature=0.7,\n",
    "    max_tokens=80\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4. Frequency_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Ensuring variety in marketing emails, preventing chatbots from repeating FAQs, generating diverse social media posts about the same product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to Observe**:\n",
    "\n",
    "- **No** penalty (0.0): Look for words or short phrases being repeated frequently within the generated paragraph.\n",
    "\n",
    "- **Moderate** penalty: Repetition should be noticeably reduced, leading to more varied sentences.\n",
    "\n",
    "- **High** penalty: The model will actively avoid repeating words, potentially leading to more complex or less natural phrasing if it has to find many synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_freq_penalty = \"\"\"\n",
    "Give me 15 different phrases that customer support executives often say. Only return the phrases.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating frequency_penalty=-1 ---\n",
      "Prompt: \"\n",
      "Give me 15 different phrases that customer support executives often say. Only return the phrases.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'seed': 42}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. \"I'd be happy to assist you further.\"\n",
       "2. \"Let me check on that for you.\"\n",
       "3. \"Can I offer you a solution?\"\n",
       "4. \"I apologize for the inconvenience.\"\n",
       "5. \"How can I make it right for you?\"\n",
       "6. \"I'd like to escalate this issue to my supervisor.\"\n",
       "7. \"Is there anything else I can help you with today?\"\n",
       "8. \"I'm going to go ahead and process that for you.\"\n",
       "9. \"Can you please provide me with your order number?\"\n",
       "10. \"I'll do my best to resolve the issue.\"\n",
       "11. \"I'd like to offer you a refund.\"\n",
       "12. \"I'm here to help you through this process.\"\n",
       "13. \"Can you confirm your email address?\"\n",
       "14. \"I'll check on the status of that for you.\"\n",
       "15. \"Is there anything else I can do to assist you?\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 4.1: Negative frequency penalty (encourages repetition)\n",
    "generate_and_display(\n",
    "    common_prompt_freq_penalty,\n",
    "    \"frequency_penalty\", -1,\n",
    "    temperature=0.7,\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating frequency_penalty=1.0 ---\n",
      "Prompt: \"\n",
      "Give me 15 different phrases that customer support executives often say. Only return the phrases.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'seed': 42}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. \"I'd be happy to assist you further.\"\n",
       "2. \"Let me check on that for you.\"\n",
       "3. \"Can you please confirm your order number?\"\n",
       "4. \"Our policy is to provide a full refund in this situation.\"\n",
       "5. \"I've located your issue and we'll take care of it right away.\"\n",
       "6. \"You're being helped by John/Emily/David today, can I get his email?\"\n",
       "7. \"Is there anything else I can help you with regarding this order?\"\n",
       "8. \"Our current processing time is 24-48 hours for expedited orders.\"\n",
       "9. \"I'd like to offer an upgrade to our premium service plan.\"\n",
       "10. \"You've been selected for our loyalty rewards program, welcome!\"\n",
       "11. \"Can you please try restarting your device and see if that resolves the issue?\"\n",
       "12. \"Our estimated delivery date is now expected on [new date].\"\n",
       "13. \"'Did you mean' to purchase [similar product] instead?\"\n",
       "14. \"I can definitely escalate this issue for priority attention.\"\n",
       "15. \"You've been charged a fee of $25 due to excessive usage.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 4.2: Moderate frequency penalty (reduced repetition)\n",
    "generate_and_display(\n",
    "    common_prompt_freq_penalty,\n",
    "    \"frequency_penalty\", 1.0, # Increased for more impact\n",
    "    temperature=0.7,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating frequency_penalty=2.0 ---\n",
      "Prompt: \"\n",
      "Give me 15 different phrases that customer support executives often say. Only return the phrases.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'seed': 42}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. \"I'd be happy to assist you further.\"\n",
       "2. \"Let's try again, shall we?\"\n",
       "3. \"Can I provide an alternative solution?\"\n",
       "4.\"What would make this issue resolve for you?\"\n",
       "5.\"Have you tried [alternative steps]?\" \n",
       "6.  No worries at all.\n",
       "7.\"'re here to help now\"\n",
       "8.\"\"I'm sorry about the inconvenience.\"\n",
       "9.'\"s a great question, let me see what I can do.\"\n",
       "10.\".Can I offer an upgrade/downgrade option?\"\n",
       "11.\"Okay, so just to confirm...\"\n",
       "12.\"Is there anything else you need assistance with today?\"\n",
       "13. \"What's your preferred resolution for this issue?\" \n",
       "14.\"'m going back into that system now\"\n",
       "15.\"\"I'm here to help resolve the problem\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 4.3: High frequency penalty (strong reduction in repetition, might affect coherence)\n",
    "generate_and_display(\n",
    "    common_prompt_freq_penalty,\n",
    "    \"frequency_penalty\", 2.0, # Max penalty for strong effect\n",
    "    temperature=0.7,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5. Presence_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Encouraging a customer service bot to explore different solutions/topics, generating comprehensive meeting minutes by covering all points, producing diverse content ideas for a campaign.\n",
    "\n",
    "**What to Observe**:\n",
    "\n",
    "- **No** penalty (0.0): The model might dwell on a few initial topics or points without moving on to others mentioned in the conceptual prompt.\n",
    "\n",
    "- **Moderate** penalty: The summary should cover a broader range of distinct action items or concepts, rather than just elaborating on the first few.\n",
    "\n",
    "- **High** penalty: The model will try hard to introduce new concepts or distinct ideas, possibly jumping between points quickly or even generating somewhat disjointed output if forced too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_pres_penalty = \"\"\"\n",
    "You are leading a quick internal meeting discussing how Generative AI can significantly improve our current sales and customer engagement processes. \n",
    "Briefly outline 4 distinct areas or strategies where GenAI could have a major impact, providing a brief example for each.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating presence_penalty=0.0 ---\n",
      "Prompt: \"\n",
      "You are leading a quick internal meeting discussing how Generative AI can significantly improve our current sales and customer engagement processes. \n",
      "Briefly outline 4 distinct areas or strategies where GenAI could have a major impact, providing a brief example for each.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's dive into the potential of Generative AI to revolutionize our sales and customer engagement processes. Here are four distinct areas where we can expect significant improvements:\n",
       "\n",
       "**1. Personalized Sales Content Generation**\n",
       "\n",
       "GenAI can help us create personalized sales content, such as emails, social media posts, and sales scripts, that cater to individual customers' needs and preferences.\n",
       "\n",
       "Example: We can use GenAI to generate a customized sales email for a lead who has shown interest in our product. The AI will analyze the customer's online behavior, previous interactions, and purchase history to craft a tailored message that resonates with their specific pain points and interests.\n",
       "\n",
       "**2. Chatbot-Powered Customer Support**\n",
       "\n",
       "GenAI-powered chatbots can help us provide faster, more efficient, and empathetic support to our customers, reducing response times and improving overall satisfaction.\n",
       "\n",
       "Example: We can train GenAI to power a chatbot that responds to common customer inquiries about our product features, pricing, and availability. The AI will learn from user interactions to improve its responses over time, providing a seamless experience for customers who prefer self-service support.\n",
       "\n",
       "**3. Customized Customer Segmentation**\n",
       "\n",
       "GenAI can help us segment our customer base into distinct groups based on their behavior, preferences, and demographic characteristics, enabling us to tailor our marketing efforts and sales strategies more effectively.\n",
       "\n",
       "Example: We can use GenAI to analyze customer data and identify patterns that reveal specific segments of our audience. For instance, the AI might detect a group of customers who are highly active on social media but have never made a purchase. By targeting this segment with targeted marketing campaigns and personalized content, we can increase engagement and conversion rates.\n",
       "\n",
       "**4. Sales Forecasting and Predictive Analytics**\n",
       "\n",
       "GenAI can help us predict sales performance by analyzing historical data, market trends, and customer behavior patterns, enabling us to make more informed forecasting decisions and optimize our sales strategies.\n",
       "\n",
       "Example: We can train GenAI on historical sales data, market research reports, and industry trends to forecast future sales revenue. The AI will identify key drivers of growth and decline, allowing us to adjust our pricing, inventory levels, and sales channels accordingly, ultimately improving our bottom line and competitiveness.\n",
       "\n",
       "These are just a few examples of the many ways in which Generative AI can enhance our sales and customer engagement processes. Do any of these areas resonate with you, or would you like to explore other applications?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 5.1: No presence penalty (might focus heavily on one or two main themes)\n",
    "generate_and_display(\n",
    "    common_prompt_pres_penalty,\n",
    "    \"presence_penalty\", 0.0,\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating presence_penalty=1.0 ---\n",
      "Prompt: \"\n",
      "You are leading a quick internal meeting discussing how Generative AI can significantly improve our current sales and customer engagement processes. \n",
      "Briefly outline 4 distinct areas or strategies where GenAI could have a major impact, providing a brief example for each.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's dive into the potential of Generative AI (GenAI) in improving our sales and customer engagement processes.\n",
       "\n",
       "**Area 1: Personalized Content Generation**\n",
       "\n",
       "* Strategy: Utilize GenAI to generate personalized content, such as blog posts, social media posts, and email campaigns, tailored to individual customers' interests and preferences.\n",
       "* Example: Our marketing team uses a GenAI tool to analyze customer data and generates customized product recommendations for each user. For instance, if a customer browses our website frequently, the system suggests relevant products based on their browsing history.\n",
       "\n",
       "**Area 2: Chatbot-Driven Customer Support**\n",
       "\n",
       "* Strategy: Leverage GenAI-powered chatbots to provide 24/7 customer support, offering immediate assistance and resolving issues quickly.\n",
       "* Example: Our company's genai-generated chatbot can respond to frequently asked questions, such as product prices and return policies. The bot also uses natural language processing (NLP) to understand the user's intent and direct them to human agents when needed.\n",
       "\n",
       "**Area 3: Data-Driven Sales Analytics**\n",
       "\n",
       "* Strategy: Harness GenAI's capabilities in data analysis to gain deeper insights into customer behavior and improve sales forecasting.\n",
       "* Example: Our genai-powered analytics tool analyzes large datasets on customer purchasing patterns, seasonality, and market trends. This enables our sales team to identify high-potential opportunities and adjust their strategies accordingly.\n",
       "\n",
       "**Area 4: Real-Time Customer Profiling**\n",
       "\n",
       "* Strategy: Develop a GenAI-driven profiling system that creates accurate, up-to-date profiles of each customer based on their interactions with the company.\n",
       "* Example: Our genai-powered customer profiling tool aggregates data from multiple sources (e.g., website sessions, social media engagement) to create rich, dynamic profiles. This information can help our sales team identify key decision-makers and tailor communications to specific personas.\n",
       "\n",
       "These are just a few examples of how GenAI can enhance our sales and customer engagement processes. By exploring these areas, we can unlock significant opportunities for growth, efficiency, and improved customer experiences."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 5.2: Moderate presence penalty (encourages covering a broader range of distinct action items)\n",
    "generate_and_display(\n",
    "    common_prompt_pres_penalty,\n",
    "    \"presence_penalty\", 1.0, # Increased for more impact\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating presence_penalty=2.0 ---\n",
      "Prompt: \"\n",
      "You are leading a quick internal meeting discussing how Generative AI can significantly improve our current sales and customer engagement processes. \n",
      "Briefly outline 4 distinct areas or strategies where GenAI could have a major impact, providing a brief example for each.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's an overview of four key areas where Generative AI (GenAI) can make significant improvements:\n",
       "\n",
       "**1. Personalized Sales Content Generation**\n",
       "\n",
       "* Strategy: Utilize GenAI to generate personalized sales content, such as email templates and social media posts, tailored to individual customers' needs and interests.\n",
       "* Example: Our marketing team uses GenAI tools to create customized sales emails that address specific pain points of our target audience. For instance, if a customer has shown interest in purchasing software for project management, the AI-generated email will highlight how our tool can help streamline their workflow.\n",
       "\n",
       "**2. Automated Customer Feedback Analysis**\n",
       "\n",
       "* Strategy: Leverage GenAI-powered Natural Language Processing (NLP) to analyze and respond to customer feedback across various channels.\n",
       "* Example: Our chatbots use GenAI algorithms to quickly scan through customer support emails, social media comments, and review sites to identify key themes and concerns. This enables us to address recurring issues proactively and improve overall customer satisfaction.\n",
       "\n",
       "**3. Predictive Content Recommendation**\n",
       "\n",
       "* Strategy: Integrate GenAI with our content management system (CMS) to predict which articles or blog posts will resonate most with specific audience segments.\n",
       "* Example: Our marketing team uses GenAI tools to analyze historical data on article engagement, sentiment, and demographics to identify top-performing topics for each region. For instance, if we notice that users in the Midwest tend to engage more with content about local business trends, our AI-powered recommendation engine suggests creating similar pieces of content.\n",
       "\n",
       "**4. Enhanced Customer Journey Mapping**\n",
       "\n",
       "* Strategy: Apply GenAI's advanced visualization capabilities to create immersive and interactive customer journey maps.\n",
       "* Example: Our sales team uses GenAI tools to generate 3D visualizations that illustrate the complexities of a potential client's buying process, highlighting where they might be most likely to encounter friction points. This allows us to tailor our approach and anticipate their needs more effectively.\n",
       "\n",
       "By implementing these strategies leveraging Generative AI, we can significantly enhance our sales and customer engagement processes while providing better value for our customers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 5.3: High presence penalty (strong encouragement for new ideas, potentially disjointed)\n",
    "generate_and_display(\n",
    "    common_prompt_pres_penalty,\n",
    "    \"presence_penalty\", 2.0, # Max penalty for strong effect\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 6. Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Ensuring chatbot responses don't continue past a natural break, extracting specific data blocks (e.g., JSON), generating bulleted lists that don't overextend, completing fill-in-the-blank forms.\n",
    "garding:\"\n",
    "\n",
    "**What to Observe**: The model's generation will immediately halt once any of the provided stop sequences appear in its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating stop=['\\n'] ---\n",
      "Prompt: \"\n",
      "Draft a quick customer service response template:\n",
      "Thank you for contacting Reliance Jio. We received your inquiry regarding:\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a draft of a quick customer service response template:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 6.1: Stopping at newline (for single-line completions)\n",
    "common_prompt_stop = \"\"\"\n",
    "Draft a quick customer service response template:\\nThank you for contacting Reliance Jio. We received your inquiry regarding:\"\"\"\n",
    "generate_and_display(\n",
    "    common_prompt_stop,\n",
    "    \"stop\", [\"\\n\"],\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating stop=['Best regards,'] ---\n",
      "Prompt: \"Draft a professional email to a client regarding their order status. \n",
      "Subject: Your Order #12345 Update\n",
      "Dear [Client Name],\n",
      "\n",
      "Your recent order #12345 has been:\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a draft of the email:\n",
       "\n",
       "Subject: Your Order #12345 Update\n",
       "\n",
       "Dear [Client Name],\n",
       "\n",
       "We are pleased to inform you that your recent order, #12345, is currently being processed and dispatched. Our team has confirmed that all necessary steps have been taken to prepare your shipment for delivery.\n",
       "\n",
       "As per our previous communication, we anticipate that your order will arrive at your doorstep within the estimated timeframe of [insert timeframe]. You can track the status of your order using the tracking number: [insert tracking number].\n",
       "\n",
       "If you have any questions or concerns regarding your order, please do not hesitate to reach out to us. We are here to assist you and ensure a seamless experience.\n",
       "\n",
       "Thank you for your patience and understanding."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 6.2: Stopping at a specific closing phrase (for structured email generation)\n",
    "common_prompt_email = \"\"\"Draft a professional email to a client regarding their order status. \n",
    "Subject: Your Order #12345 Update\\nDear [Client Name],\\n\\nYour recent order #12345 has been:\"\"\"\n",
    "generate_and_display(\n",
    "    common_prompt_email,\n",
    "    \"stop\", [\"Best regards,\"], # Model will stop before adding the closing\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating stop=['4.'] ---\n",
      "Prompt: \"List 3 benefits of our premium AI/ML package:\n",
      "1. Enhanced security\n",
      "2. Real-time analytics\n",
      "3.\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here is the completed list:\n",
       "\n",
       "**Benefits of Our Premium AI/ML Package:**\n",
       "\n",
       "1. **Enhanced Security**: Our advanced AI-powered threat detection and prevention capabilities protect your business from sophisticated cyber threats, ensuring sensitive data remains secure.\n",
       "2. **Real-time Analytics**: Get instant insights into customer behavior, market trends, and operational performance with our cutting-edge analytics platform, empowering data-driven decision-making.\n",
       "3. **Predictive Modeling and Automation**: Leverage our machine learning algorithms to predict customer churn, optimize resource allocation, and automate repetitive tasks, freeing up your team to focus on strategic initiatives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 6.3: Stopping after a list item (for controlled list generation)\n",
    "common_prompt_list = \"List 3 benefits of our premium AI/ML package:\\n1. Enhanced security\\n2. Real-time analytics\\n3.\"\n",
    "generate_and_display(\n",
    "    common_prompt_list,\n",
    "    \"stop\", [\"4.\"], # Stop before generating the 4th item (assuming it would start with 4.)\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 7. Seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Reproducible content for A/B testing (ensuring differences are due to prompt, not randomness), consistent chatbot behavior for quality assurance, debugging prompt engineering, generating consistent test data.\n",
    "\n",
    "**What to Observe**:\n",
    "\n",
    "- **Same** seed: The outputs for the multiple runs should be identical or extremely close.\n",
    "- **Different** seed: The outputs, while still relevant to the prompt, should be distinct from each other across different seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_seed = \"Generate a marketing slogan for a new eco-friendly cleaning product. Only return the slogan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing reproducibility with the SAME seed (42) ---\n",
      "\n",
      "--- Demonstrating seed=42 ---\n",
      "Prompt: \"Generate a marketing slogan for a new eco-friendly cleaning product. Only return the slogan.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 20}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Clean with a Clear Conscience, Naturally.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Clean with a Clear Conscience, Naturally.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Clean with a Clear Conscience, Naturally.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 7.1: Same seed, same prompt, same parameters (for reproducible slogan generation for testing)\n",
    "print(\"\\n--- Testing reproducibility with the SAME seed (42) ---\")\n",
    "generate_and_display(\n",
    "    common_prompt_seed,\n",
    "    \"seed\", 42,\n",
    "    num_runs=3, # Run multiple times to show consistency\n",
    "    temperature=0.7,\n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing different seeds for DIVERSE slogan options ---\n",
      "\n",
      "--- Demonstrating seed=101 ---\n",
      "Prompt: \"Generate a marketing slogan for a new eco-friendly cleaning product. Only return the slogan.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 20}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Sparkle with a Clear Conscience, Clean with Green Intent.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 7.2: Different seeds, same prompt, same parameters (to get diverse options for review)\n",
    "print(\"\\n--- Testing different seeds for DIVERSE slogan options ---\")\n",
    "generate_and_display(\n",
    "    common_prompt_seed,\n",
    "    \"seed\", 101,\n",
    "    num_runs=1, # One run for each distinct seed is enough to show variety\n",
    "    temperature=0.7,\n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating seed=202 ---\n",
      "Prompt: \"Generate a marketing slogan for a new eco-friendly cleaning product. Only return the slogan.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 20}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Clean with a Clear Conscience, Inside and Out\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 7.3: Different seeds, same prompt, same parameters (to get diverse options for review)\n",
    "generate_and_display(\n",
    "    common_prompt_seed,\n",
    "    \"seed\", 202,\n",
    "    num_runs=1,\n",
    "    temperature=0.7,\n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating seed=303 ---\n",
      "Prompt: \"Generate a marketing slogan for a new eco-friendly cleaning product. Only return the slogan.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 20}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Cleaning your home, greening our planet.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 7.4: Different seeds, same prompt, same parameters (to get diverse options for review)\n",
    "generate_and_display(\n",
    "    common_prompt_seed,\n",
    "    \"seed\", 303,\n",
    "    num_runs=1,\n",
    "    temperature=0.7,\n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
