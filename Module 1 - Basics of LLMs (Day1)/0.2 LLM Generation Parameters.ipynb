{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Generation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this! - What does this do?\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Chat Completions API (most widely used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OpenAI` Official documentation is here - https://platform.openai.com/docs/api-reference/chat/create Refer to this for a full set of parameters that we'll later explore. \n",
    "\n",
    "The most used parameters are as listed below, with definitions from official `OpenAI` API documentation. These are considered \"common\" because they fundamentally control how an autoregressive language model samples tokens to generate output. \n",
    "\n",
    "- **max_tokens** (only *max_completion_tokens* is supported by o1 series; no default): The maximum number of completion tokens that may be used over the course of the run. The run will make a best effort to use only the number of completion tokens specified, across multiple turns of the run. If the run exceeds the number of completion tokens specified, the run will end with status `incomplete`.\n",
    "- **temperature** (*1 by default*): What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.\n",
    "- **top_p** (*1 by default*): An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "We generally recommend altering this or temperature but not both.\n",
    "- **top_k**: not available in OpenAI client library\n",
    "- **frequency_penalty** (*0 by default*): Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
    "- **presence_penalty** (*0 by default*): Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "- **stop** (*null by default*): Not supported with latest reasoning models o3 and o4-mini. Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic's Messages API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to note that Anthropic's `Messages` API contains similar parameters except a few differences:\n",
    "\n",
    "- **max_tokens** is used as is\n",
    "- **temperature** has a value between 0 and 1\n",
    "- **top_p** is as is\n",
    "- **top_k** is also available: Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\n",
    "- **frequency_penalty** and **presence_penalty** parameters are not available.\n",
    "- **stop_sequences** is used instead of **stop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some important considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Exact naming conventions of parameters can vary, but functionality is same (like we saw with Anthropic's API).\n",
    "- The optimal range or impact of a parameter might differ between models, even if general principle is same. A temperature of 1.0 might be very wild for one model but only moderately creative for another.\n",
    "- Differences between different parameter configurations will be more prominent in larger LLMs, as with smaller LLMs, the \"spread\" of probabilities for the next token might not be as wide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting temperature vs. top_p for different use cases: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "| Use Case               | Temperature | Top_p | Description                                                                                        |\n",
    "| :--------------------- | :---------- | :---- | :------------------------------------------------------------------------------------------------- |\n",
    "| Code Generation        | 0.2         | 0.1   | Generates code that adheres to established patterns and conventions. Output is more deterministic and focused. Useful for generating syntactically correct code. |\n",
    "| Creative Writing       | 0.7         | 0.8   | Generates creative and diverse text for storytelling. Output is more exploratory and less constrained by patterns. |\n",
    "| Chatbot Responses      | 0.5         | 0.5   | Generates conversational responses that balance coherence and diversity. Output is more natural and engaging. |\n",
    "| Code Comment Generation | 0.3         | 0.2   | Generates code comments that are more likely to be concise and relevant. Output is more deterministic and adheres to conventions. |\n",
    "| Data Analysis Scripting | 0.2         | 0.1   | Generates data analysis scripts that are more likely to be correct and efficient. Output is more deterministic and focused. |\n",
    "| Exploratory Code Writing | 0.6         | 0.7   | Generates code that explores alternative solutions and creative approaches. Output is less constrained by established patterns. |\n",
    "\n",
    "References from here: https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api/172683/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Use either Llama3.2 (3B) with Ollama\n",
    "!ollama pull llama3.2    \n",
    "\n",
    "# Initialize the OpenAI client to connect to Ollama\n",
    "llm_api = OpenAI(base_url='http://localhost:11434/v1', \n",
    "                 api_key='ollama') # note that llm_api is a good variable name, because the LLM is indeed exposed locally as an API by Ollama!\n",
    "\n",
    "# Define the model to use\n",
    "model_name = \"llama3.2\" # Ensure this matches the pulled model in Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Or use DeepSeek-R1-Distill-Llama-70B from Groq\n",
    "# llm_api = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "\n",
    "# # Define the model to use\n",
    "# model_name = \"deepseek-r1-distill-llama-70b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for text generation\n",
    "def generate_and_display(prompt, param_name, param_value, num_runs=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to make a call to the LLM and display results.\n",
    "    Runs multiple times for stochastic parameters to highlight differences.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Demonstrating {param_name}={param_value} ---\")\n",
    "    print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "    print(f\"Other consistent parameters: {kwargs}\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful and concise AI assistant for a business.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        print(f\"\\n--- Run {i+1} ---\")\n",
    "        try:\n",
    "            response = llm_api.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                **{param_name: param_value, **kwargs}\n",
    "            )\n",
    "            print(\"Generated Text:\")\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            display(Markdown(answer)) # .strip() removes leading/trailing whitespace for cleaner output\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Ensure Ollama is running and the model is pulled. Skipping further runs for this parameter setting.\")\n",
    "            break # Exit loop if an error occurs\n",
    "\n",
    "    print(\"\\n--- End Demonstration for this parameter setting ---\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Content generation (marketing copy, ad variations, blog post ideas), creative brainstorming for product features, unique customer engagement messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to expect:**\n",
    "- **Low Temperature**: Look for very similar, factual, and direct outputs across runs. Less \"fluff,\" more to the point.\n",
    "    - E.g., Consistent, factual product bullet points for a catalog\n",
    "- **Medium Temperature**: Outputs will start showing more natural language, some variation in phrasing, but still coherent.\n",
    "    - E.g., Engaging marketing copy for a landing page\n",
    "- **High Temperature**: Expect more varied vocabulary, potentially more metaphorical or unusual phrasing, and noticeable differences between runs. It might sometimes deviate slightly from the core prompt if pushed too high.\n",
    "    - E.g., Brainstorming unique ad slogans or creative concepts for a campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_temp = \"\"\"\n",
    "Merck is heavily investing in Agentic AI and AI Gateways to personalize healthcare for millions of patients. \n",
    "Envision a new Agentic AI-powered service, \"Merck Vitals,\" that leverages patient data and medical knowledge.\n",
    "First, describe the service focusing on its innovative features and how it will enhance daily life for a typical user.\n",
    "Then, think about the role of AI gateway in enabling a production-ready solution.\n",
    "Keep the answer less than 70 tokens.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating temperature=0.1 ---\n",
      "Prompt: \"\n",
      "Merck is heavily investing in Agentic AI and AI Gateways to personalize healthcare for millions of patients. \n",
      "Envision a new Agentic AI-powered service, \"Merck Vitals,\" that leverages patient data and medical knowledge.\n",
      "First, describe the service focusing on its innovative features and how it will enhance daily life for a typical user.\n",
      "Then, think about the role of AI gateway in enabling a production-ready solution.\n",
      "Keep the answer less than 70 tokens.\"\n",
      "Other consistent parameters: {'max_completion_tokens': 70}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Merck Vitals: Personalized Healthcare Service**\n",
       "\n",
       "Merck Vitals is an Agentic AI-powered service that empowers patients to take control of their health. Key features include:\n",
       "\n",
       "* **Personalized Health Profiles**: AI-driven analysis of patient data, medical history, and lifestyle habits.\n",
       "* **Predictive Insights**: Proactive alerts for potential health risks and preventive measures.\n",
       "* **Medication Optimization**: AI-suggested medication regimens based on individual needs.\n",
       "\n",
       "**AI Gateway Enabling Production-Ready Solution**\n",
       "\n",
       "The AI gateway plays a crucial role in integrating Merck Vitals with various healthcare systems, ensuring seamless data exchange and secure patient data management. The gateway enables:\n",
       "\n",
       "* **Data Standardization**: Normalizing patient data for analysis and processing.\n",
       "* **Integration with Wearables & Devices**: Connecting patients' wearable devices to the platform.\n",
       "* **Secure Data Storage**: Safeguarding sensitive patient information.\n",
       "\n",
       "By leveraging these innovative features, Merck Vitals enhances daily life for typical users by providing proactive guidance, empowering informed decision-making, and improving overall health outcomes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Merck Vitals: Personalized Healthcare Service**\n",
       "\n",
       "Merck Vitals is an Agentic AI-powered service that empowers patients to take control of their health. Key features include:\n",
       "\n",
       "* **Personalized Health Profiles**: AI-driven analysis of patient data, medical history, and lifestyle habits creates tailored recommendations for wellness and disease prevention.\n",
       "* **Predictive Insights**: Advanced analytics identify potential health risks, enabling proactive interventions and early treatment.\n",
       "* **Virtual Coaching**: AI-powered chatbots offer guidance on healthy habits, medication adherence, and symptom management.\n",
       "\n",
       "**AI Gateway Enabling Production-Ready Solution**\n",
       "\n",
       "The AI gateway plays a crucial role in integrating Merck Vitals with various healthcare systems, ensuring seamless data exchange and secure patient data protection. The gateway enables:\n",
       "\n",
       "* **Data Standardization**: Normalizing patient data for analysis and processing.\n",
       "* **Integration with Wearables & Devices**: Connecting patients' wearable devices and medical equipment to the platform.\n",
       "* **Secure Data Transmission**: Ensuring HIPAA compliance and protecting sensitive patient information.\n",
       "\n",
       "By leveraging AI gateways, Merck Vitals can provide a comprehensive, production-ready solution that revolutionizes personalized healthcare for millions of patients."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Merck Vitals: Personalized Healthcare Service**\n",
       "\n",
       "Merck Vitals is an Agentic AI-powered service that empowers patients to take control of their health. Key features include:\n",
       "\n",
       "* **Personalized Health Profiles**: AI-driven analysis of patient data, medical history, and lifestyle habits creates a tailored profile.\n",
       "* **Predictive Insights**: AI forecasts potential health risks and recommends preventive measures.\n",
       "* **Medication Optimization**: AI suggests optimal medication regimens based on individual needs.\n",
       "\n",
       "**AI Gateway Enabling Production-Ready Solution**\n",
       "\n",
       "The AI gateway plays a crucial role in integrating patient data from various sources, ensuring seamless communication between devices, and enabling real-time updates. This enables Merck Vitals to provide:\n",
       "\n",
       "* **Real-time Insights**: Patients receive immediate feedback on their health status.\n",
       "* **Streamlined Care**: Healthcare providers have access to comprehensive patient profiles for informed decision-making.\n",
       "\n",
       "By leveraging AI gateways, Merck Vitals can deliver a production-ready solution that revolutionizes personalized healthcare."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 1.1: Low Temperature\n",
    "generate_and_display(\n",
    "    common_prompt_temp,\n",
    "    \"temperature\", 0.1,\n",
    "    num_runs=3, # Run multiple times to show consistency\n",
    "    max_completion_tokens=70\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating temperature=0.7 ---\n",
      "Prompt: \"\n",
      "Merck is heavily investing in Agentic AI and AI Gateways to personalize healthcare for millions of patients. \n",
      "Envision a new Agentic AI-powered service, \"Merck Vitals,\" that leverages patient data and medical knowledge.\n",
      "First, describe the service focusing on its innovative features and how it will enhance daily life for a typical user.\n",
      "Then, think about the role of AI gateway in enabling a production-ready solution.\n",
      "Keep the answer less than 70 tokens.\"\n",
      "Other consistent parameters: {'max_completion_tokens': 70}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Merck Vitals**\n",
       "\n",
       "Merck Vitals is an Agentic AI-powered service that empowers patients to take control of their health. Its innovative features include:\n",
       "\n",
       "* Personalized health dashboards with real-time insights\n",
       "* AI-driven risk prediction and prevention recommendations\n",
       "* Virtual consultations with healthcare professionals\n",
       "* Integrated medication management and adherence tracking\n",
       "\n",
       "These features enhance daily life by providing patients with actionable intelligence, increasing confidence in self-care, and improving overall well-being.\n",
       "\n",
       "**AI Gateway Role**\n",
       "\n",
       "The AI gateway plays a crucial role in enabling a production-ready solution for Merck Vitals. It connects to various data sources, aggregates patient information, and applies machine learning algorithms to provide actionable insights. The gateway ensures seamless integration of disparate systems, providing a secure and scalable platform for advanced analytics and decision-making."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Merck Vitals: Personalized Healthcare**\n",
       "\n",
       "Merck Vitals is an Agentic AI-powered service that empowers patients to take control of their health. Innovative features include:\n",
       "\n",
       "* **Personalized Medicine Plans**: AI-driven recommendations based on individual medical history, lifestyle, and genetic profiles.\n",
       "* **Real-time Health Insights**: Continuous monitoring and analysis of vital signs, medication interactions, and predictive analytics for early disease detection.\n",
       "* **Smart Health Coaching**: AI-assisted guidance for healthy habits, wellness tips, and medication adherence.\n",
       "\n",
       "**AI Gateway**\n",
       "\n",
       "The Merck Vitals AI gateway seamlessly integrates patient data from various sources, ensuring a comprehensive view of each individual's health. This gateway enables:\n",
       "\n",
       "* Secure data exchange between healthcare providers and patients.\n",
       "* Automated data standardization and analytics.\n",
       "* Advanced security measures to protect sensitive medical information.\n",
       "\n",
       "By harnessing the power of Agentic AI and AI gateways, Merck Vitals revolutionizes patient engagement and personalized care, enhancing daily life and improving health outcomes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Merck Vitals: Personalized Healthcare**\n",
       "\n",
       "Merck Vitals is an Agentic AI-powered service that empowers patients to take control of their health. Key features include:\n",
       "\n",
       "* **Personalized Health Insights**: AI-driven analysis of patient data, medical history, and lifestyle habits provides actionable recommendations for optimal well-being.\n",
       "* **Predictive Analytics**: Advanced algorithms identify potential health risks, enabling early interventions and preventive care.\n",
       "* **Virtual Care Coaching**: AI-powered chatbots offer personalized guidance on healthy behaviors, medication adherence, and disease management.\n",
       "\n",
       "**AI Gateway Role**\n",
       "\n",
       "The AI gateway plays a crucial role in enabling Merck Vitals by:\n",
       "\n",
       "* Integrating with various electronic health records (EHRs) and wearables\n",
       "* Providing real-time data processing and analytics capabilities\n",
       "* Ensuring seamless communication between patients, healthcare providers, and the AI system"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 1.2: Medium Temperature\n",
    "generate_and_display(\n",
    "    common_prompt_temp,\n",
    "    \"temperature\", 0.7,\n",
    "    num_runs=3, # Run multiple times to show controlled variation\n",
    "    max_completion_tokens=70\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating temperature=1.5 ---\n",
      "Prompt: \"\n",
      "Merck is heavily investing in Agentic AI and AI Gateways to personalize healthcare for millions of patients. \n",
      "Envision a new Agentic AI-powered service, \"Merck Vitals,\" that leverages patient data and medical knowledge.\n",
      "First, describe the service focusing on its innovative features and how it will enhance daily life for a typical user.\n",
      "Then, think about the role of AI gateway in enabling a production-ready solution.\n",
      "Keep the answer less than 70 tokens.\"\n",
      "Other consistent parameters: {'max_completion_tokens': 70}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Introducing Merck Vitals, an Agentic AI-powered service that harmonizes personalization with healthcare. This innovative service features:\n",
       "\n",
       "* Data-driven insights to foster meaningful patient-provider conversations\n",
       "* Real-time monitoring of vital signs and medication management\n",
       "* Personalized treatment plans tailored to each patient's unique needs\n",
       "* AI-gated self-care tools for seamless life coordination\n",
       "\n",
       "To bring this vision to production, Merck implements AI gateways that manage data privacy, security, and interoperability. By integrating Agentic AI with the gateway's robust architecture, patients can enjoy an intuitive, user-friendly experience while ensuring rigorous cybersecurity and regulatory compliance worldwide"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Merck Vitals: Empowering Personalized Healthcare**\n",
       "\n",
       "Merck Vitals is an Agentic AI-powered service that combines patient data and medical expertise to provide tailored health insights and recommendations. Innovative features:\n",
       "\n",
       "* **Proactive health tracking**: AI-analyzes user data (wearables, sensors, medical records) to identify early warning signs of potential illnesses.\n",
       "* **Personalized health plans**: Offers AI-generated strategies to mitigate or reverse adverse conditions, incorporating genomic information and lifestyle factors.\n",
       "* **Enhanced medication efficacy**: Intelligent refashioning of treatments based on a patient's response history.\n",
       "\n",
       "AI Gateway acts as an enabler by:\n",
       "\n",
       "* Securing seamless clinical data integration\n",
       "* Processing large datasets simultaneously for efficient analysis.\n",
       "* Validating AI-derived suggestions through peer-reviewed medical sources, ensuring quality assurance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Merck Vitals:** A Personalized Health Service\n",
       "\n",
       "Merck Vitals is an agentic AI-powered service that harnesses patient data and medical knowledge to Provide tailored wellness recommendations, proactive alerts for preventable conditions, and insightful analytics for health equity assessment.\n",
       "\n",
       "**Innovative Features:**\n",
       "\n",
       "1. **Patient-centric risk assessment:** AI predicts individual health risks, prompting targeted interventions.\n",
       "2. **Symptom monitoring ecosystem:** Track symptoms through voice chatbots or wearables, accessing expert-approved analysis.\n",
       "3. **Peer support networks:** Engage in secure communities based on diagnoses and interests for mutual support.\n",
       "\n",
       "**AI Gateway Integration:**\n",
       "An optimized AI gateway facilitates seamless flow of integrated clinical, genomic, and lifestyle data to foster more comprehensive predictive models enabling personalized health journeys"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 1.3: High Temperature\n",
    "generate_and_display(\n",
    "    common_prompt_temp,\n",
    "    \"temperature\", 1.5, # Pushing temperature higher for more dramatic effect\n",
    "    num_runs=3, # Run multiple times to show increased randomness\n",
    "    max_completion_tokens=70\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Chatbot response length control, email subject line generation, tweet generation, summarizing lengthy documents for internal reports, generating specific-length ad copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to expect**: \n",
    "\n",
    "- The generated output will strictly adhere to the `max_tokens` limit, regardless of whether the summary is complete.\n",
    "- Depending on the use case, the `max_tokens` parameter can be set:\n",
    "    - Very few tokens (e.g., for a headline or quick alert)\n",
    "    - Moderate tokens (e.g., for an executive summary or internal Slack message)\n",
    "    - More tokens (e.g., for a detailed summary section in a newsletter or report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_max_tokens = \"\"\"\n",
    "Imagine you're a data scientist working with an IPL team. \n",
    "Describe how Generative AI could be used to create novel training drills or strategic simulations for batsmen, considering the unique challenges of Indian pitches and varied bowling styles.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating max_tokens=15 ---\n",
      "Prompt: \"\n",
      "Imagine you're a data scientist working with an IPL team. \n",
      "Describe how Generative AI could be used to create novel training drills or strategic simulations for batsmen, considering the unique challenges of Indian pitches and varied bowling styles. Complete response in less than 15 tokens.\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As a data scientist, I'd leverage Generative AI to:\n",
       "\n",
       "1."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 2.1: Very few tokens\n",
    "generate_and_display(\n",
    "    common_prompt_max_tokens + \" Complete response in less than 15 tokens.\",\n",
    "    \"max_tokens\", 15,\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating max_tokens=50 ---\n",
      "Prompt: \"\n",
      "Imagine you're a data scientist working with an IPL team. \n",
      "Describe how Generative AI could be used to create novel training drills or strategic simulations for batsmen, considering the unique challenges of Indian pitches and varied bowling styles.\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As a data scientist working with an IPL team, I'd love to explore the potential of Generative AI (GA) in creating novel training drills and strategic simulations for batsmen. Here's how GA can be leveraged:\n",
       "\n",
       "**Data Collection**\n",
       "\n",
       "To"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 2.2: Moderate tokens\n",
    "generate_and_display(\n",
    "    common_prompt_max_tokens,\n",
    "    \"max_tokens\", 50,\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating max_tokens=150 ---\n",
      "Prompt: \"\n",
      "Imagine you're a data scientist working with an IPL team. \n",
      "Describe how Generative AI could be used to create novel training drills or strategic simulations for batsmen, considering the unique challenges of Indian pitches and varied bowling styles.\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As a data scientist working with an IPL team, I'd love to explore the potential of Generative AI (GAI) in creating novel training drills and strategic simulations for batsmen. Here's how:\n",
       "\n",
       "**Data Collection and Analysis**\n",
       "\n",
       "To leverage GAI effectively, we'll need access to large datasets containing information on:\n",
       "\n",
       "1. Batting performances and statistics\n",
       "2. Bowling styles, including types (fast, spin, etc.), deliveries (short-pitched, yorkers, etc.), and variations (leg-spin, off-spin, etc.)\n",
       "3. Pitch conditions, including surface type (dry, damp, etc.), weather conditions (temperature, humidity, etc.), and ball tracking data\n",
       "4. Historical matches and key events (dismissals, boundaries,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 2.3: More tokens\n",
    "generate_and_display(\n",
    "    common_prompt_max_tokens,\n",
    "    \"max_tokens\", 150,\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating max_tokens=1000 ---\n",
      "Prompt: \"\n",
      "Imagine you're a data scientist working with an IPL team. \n",
      "Describe how Generative AI could be used to create novel training drills or strategic simulations for batsmen, considering the unique challenges of Indian pitches and varied bowling styles.\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As a data scientist working with an IPL team, I'd love to explore the potential of Generative AI in creating novel training drills and strategic simulations for batsmen. Here's how:\n",
       "\n",
       "**Training Drills:**\n",
       "\n",
       "1. **Adaptive Batting Drills:** Utilize Generative AI to create customized batting drills based on individual batsmen's strengths, weaknesses, and playing styles. The AI can analyze data from past matches, identify patterns, and generate new drill scenarios that cater to the batsman's needs.\n",
       "2. **Pitch-Specific Training:** Leverage Generative AI to design training drills tailored to specific Indian pitches (e.g., Eden Gardens, Wankhede Stadium, or Feroz Shah Kotla). The AI can analyze data on ball movement, bounce, and spin patterns for each pitch to create drills that mimic the unique challenges of those surfaces.\n",
       "3. **Batting Against Different Bowlers:** Use Generative AI to generate training scenarios where batsmen face various bowling styles (e.g., fast bowler, spinner, or all-rounder). The AI can analyze data on bowlers' strengths, weaknesses, and tactics to create drills that challenge the batsman in different ways.\n",
       "\n",
       "**Strategic Simulations:**\n",
       "\n",
       "1. **Batting Lineup Optimization:** Employ Generative AI to optimize batting lineups based on team performance, opponent analysis, and player availability. The AI can generate simulations of different batting combinations, taking into account factors like wicket-keeping, fielding positions, and batting order.\n",
       "2. **Pitch Selection and Strategy:** Utilize Generative AI to analyze data on pitch conditions, opposition strengths, and weather forecasts to predict optimal pitch selection for the team. The AI can also simulate different batting strategies (e.g., playing aggressively or defensively) based on the expected conditions.\n",
       "3. **Adaptive Batting Plans:** Leverage Generative AI to create adaptive batting plans that adjust to changing game situations. For example, if the opposition is losing wickets, the AI can generate a plan that focuses on scoring quickly and taking advantage of their vulnerability.\n",
       "\n",
       "**Benefits:**\n",
       "\n",
       "1. **Increased Efficiency:** Generative AI can significantly reduce the time spent on traditional training methods, allowing the team to focus on more high-intensity, results-driven sessions.\n",
       "2. **Improved Accuracy:** By analyzing vast amounts of data, Generative AI can identify patterns and trends that human analysts might miss, providing a more accurate assessment of batsmen's strengths and weaknesses.\n",
       "3. **Enhanced Player Development:** The personalized training drills and strategic simulations generated by Generative AI can help players develop their skills at an accelerated pace, leading to improved overall performance.\n",
       "\n",
       "**Technical Requirements:**\n",
       "\n",
       "1. **Data Collection:** Access to large datasets on batting performances, bowling styles, pitch conditions, and weather forecasts.\n",
       "2. **AI Algorithm:** Adoption of a suitable Generative AI algorithm (e.g., GANs or Variational Autoencoders) that can process and analyze the collected data effectively.\n",
       "3. **Cloud Computing:** Utilization of cloud computing resources to support the processing and simulation requirements of Generative AI.\n",
       "\n",
       "By leveraging Generative AI in this way, the IPL team can unlock new levels of innovation and effectiveness in batting training and strategic planning, ultimately giving them a competitive edge in the high-stakes world of Indian Premier League cricket."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Example 2.4: Even more max_tokens\n",
    "generate_and_display(\n",
    "    common_prompt_max_tokens,\n",
    "    \"max_tokens\", 1000,\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Top_p (Nucleus sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Generating diverse customer service responses (without going off-topic), creating varied product names, generating slightly different content variations for A/B testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to Observe**:\n",
    "\n",
    "- **Low** top_p: Outputs will be very similar across runs, often picking the most statistically common words/phrases. Less variation in phrasing.\n",
    "\n",
    "- **Medium** top_p: Some variations in phrasing will emerge, but still within a coherent and expected range.\n",
    "\n",
    "- **High** top_p: More diverse and potentially more \"creative\" word choices and phrasing, showing greater variation across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_top_p = \"\"\"\n",
    "A customer is asking about our new premium coffee blend, that rivals Starbucks, Peets Coffee and Cafe Coffee Day. \n",
    "Provide a welcoming response that also highlights its unique flavor notes.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating top_p=0.1 ---\n",
      "Prompt: \"\n",
      "A customer is asking about our new premium coffee blend, that rivals Starbucks, Peets Coffee and Cafe Coffee Day. \n",
      "Provide a welcoming response that also highlights its unique flavor notes.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 80}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Welcome to [Your Business Name]! We're thrilled you're interested in trying our new premium coffee blend. I'd be happy to tell you more about it.\n",
       "\n",
       "Our expertly crafted blend is indeed a game-changer, and we're confident it will rival the best of the big names like Starbucks, Peet's Coffee, and Cafe Coffee Day. What sets us apart, however, is"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Welcome to [Your Business Name]! We're thrilled you're interested in trying our new premium coffee blend. I'd be happy to tell you more about it.\n",
       "\n",
       "Our expertly crafted blend is indeed a game-changer, and we're confident it will rival the best of the big names like Starbucks, Peet's Coffee, and Cafe Coffee Day. What sets us apart, however, is"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Welcome to [Your Business Name]! We're thrilled you're interested in trying our new premium coffee blend. I'd be happy to tell you more about it.\n",
       "\n",
       "Our expertly crafted blend is indeed a game-changer, and we're confident it will rival the best of the big names like Starbucks, Peet's Coffee, and Cafe Coffee Day. What sets us apart, however, is"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 3.1: Low top_p (for a very standard, consistent customer service greeting)\n",
    "generate_and_display(\n",
    "    common_prompt_top_p,\n",
    "    \"top_p\", 0.1,\n",
    "    num_runs=3,\n",
    "    temperature=0.7,\n",
    "    max_tokens=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating top_p=0.5 ---\n",
      "Prompt: \"\n",
      "A customer is asking about our new premium coffee blend, that rivals Starbucks, Peets Coffee and Cafe Coffee Day. \n",
      "Provide a welcoming response that also highlights its unique flavor notes.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 80}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Welcome to [Your Business Name]! We're thrilled you're interested in trying our new premium coffee blend. Our expert roasters have crafted a truly exceptional cup that's sure to satisfy even the most discerning palates.\n",
       "\n",
       "Our signature blend, 'Elevate,' is carefully selected from the finest Arabica beans sourced from around the world. What sets it apart is its unique flavor profile,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Thank you for stopping by! We're thrilled to introduce our newest addition - a premium coffee blend that's sure to tantalize your taste buds. Our expertly crafted blend is designed to rival the best, and we think it just might give Starbucks, Peets Coffee, and Cafe Coffee Day a run for their money.\n",
       "\n",
       "This exceptional blend boasts a rich, smooth flavor profile with notes of dark chocolate"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a warm and inviting response to the customer's inquiry:\n",
       "\n",
       "\"Thank you for your interest in our new premium coffee blend! We're thrilled to introduce you to our latest creation, carefully crafted to rival the best of the industry - Starbucks, Peets Coffee, and Cafe Coffee Day.\n",
       "\n",
       "Our unique blend is a masterful fusion of expertly sourced Arabica beans from various regions around the world."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 3.2: Medium top_p (for a friendly, slightly varied customer service interaction)\n",
    "generate_and_display(\n",
    "    common_prompt_top_p,\n",
    "    \"top_p\", 0.5,\n",
    "    num_runs=3,\n",
    "    temperature=0.7,\n",
    "    max_tokens=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating top_p=0.99 ---\n",
      "Prompt: \"\n",
      "A customer is asking about our new premium coffee blend, that rivals Starbucks, Peets Coffee and Cafe Coffee Day. \n",
      "Provide a welcoming response that also highlights its unique flavor notes.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 80}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Welcome to our shop! We're thrilled you're interested in trying our newest premium coffee blend, which has been making waves among coffee connoisseurs. Our expertly crafted roast is indeed comparable to some of the world's top brands like Starbucks, Peets Coffee, and Cafe Coffee Day.\n",
       "\n",
       "This unique blend is carefully curated from a selection of rare and exotic beans sourced from small-batch farms around"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Welcome to [Your Business Name]! We're thrilled to have you try our new premium coffee blend, carefully crafted to rival the best of the industry. Our expert roasters have worked tirelessly to create a truly exceptional cup, with a rich and smooth flavor profile that's sure to delight both coffee connoisseurs and newcomers alike.\n",
       "\n",
       "Our signature blend is made from a selection of rare and exotic beans"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "We're thrilled to introduce you to our newest premium coffee blend - a game-changer for coffee connoisseurs like yourself! Our expert roasters have carefully crafted this unique blend, inspired by the finest Arabica beans from around the world.\n",
       "\n",
       "Our premium coffee blend is designed to rival the best of the industry's biggest names: Starbucks, Peets Coffee, and Cafe Coffee Day. With its rich"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 3.3: High top_p (for brainstorming diverse opening lines for marketing outreach)\n",
    "generate_and_display(\n",
    "    common_prompt_top_p,\n",
    "    \"top_p\", 0.99, # Pushing higher for more noticeable effect\n",
    "    num_runs=3,\n",
    "    temperature=0.7,\n",
    "    max_tokens=80\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Frequency_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Ensuring variety in marketing emails, preventing chatbots from repeating FAQs, generating diverse social media posts about the same product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to Observe**:\n",
    "\n",
    "- **No** penalty (0.0): Look for words or short phrases being repeated frequently within the generated paragraph.\n",
    "\n",
    "- **Moderate** penalty: Repetition should be noticeably reduced, leading to more varied sentences.\n",
    "\n",
    "- **High** penalty: The model will actively avoid repeating words, potentially leading to more complex or less natural phrasing if it has to find many synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_freq_penalty = \"\"\"\n",
    "Give me 15 different phrases that customer support executives often say. Only return the phrases.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating frequency_penalty=-1 ---\n",
      "Prompt: \"\n",
      "Give me 15 different phrases that customer support executives often say. Only return the phrases.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'seed': 42}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. \"I'd be happy to assist you further.\"\n",
       "2. \"Can you please provide more details?\"\n",
       "3. \"I apologize for the inconvenience.\"\n",
       "4. \"Let me check on that for you.\"\n",
       "5. \"Our policy is as follows...\"\n",
       "6. \"I understand your concern, and I'm here to help.\"\n",
       "7. \"I'll do my best to resolve this issue.\"\n",
       "8. \"Can you please confirm your order number?\"\n",
       "9. \"I'll escalate this to my supervisor.\"\n",
       "10. \"I apologize for the delay.\"\n",
       "11. \"I'd like to offer a solution.\"\n",
       "12. \"I'll need to verify some information.\"\n",
       "13. \"I'll provide you with an update.\"\n",
       "14. \"Is there anything else I can assist you with?\"\n",
       "15. \"I'll need to authenticate your account.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 4.1: Negative frequency penalty (encourages repetition)\n",
    "generate_and_display(\n",
    "    common_prompt_freq_penalty,\n",
    "    \"frequency_penalty\", -1,\n",
    "    temperature=0.7,\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating frequency_penalty=1.0 ---\n",
      "Prompt: \"\n",
      "Give me 15 different phrases that customer support executives often say. Only return the phrases.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'seed': 42}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. \"I'd be happy to assist you further.\"\n",
       "2. \"Can you please provide more details?\"\n",
       "3. \"Let me see what I can do.\"\n",
       "4. \"Our policy is...\"\n",
       "5. \"I apologize for the inconvenience.\"\n",
       "6. \"Would you like me to escalate this issue?\"\n",
       "7. \"That's not possible at this time.\"\n",
       "8. \"May I ask a few questions?\"\n",
       "9. \"Is there anything else I can help with today?\"\n",
       "10. \"Can you confirm your order number please?\"\n",
       "11. \"I'll check on that for you immediately.\"\n",
       "12. \"Our current status is...\"\n",
       "13. \"What was the issue again?\"\n",
       "14. \"Would you prefer to resolve this now or at a later time?\"\n",
       "15. \"Is there anything else I can do right now?\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 4.2: Moderate frequency penalty (reduced repetition)\n",
    "generate_and_display(\n",
    "    common_prompt_freq_penalty,\n",
    "    \"frequency_penalty\", 1.0, # Increased for more impact\n",
    "    temperature=0.7,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating frequency_penalty=2.0 ---\n",
      "Prompt: \"\n",
      "Give me 15 different phrases that customer support executives often say. Only return the phrases.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'seed': 42}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. \"I'd be happy to assist you further.\"\n",
       "2. \"Can I provide an alternative solution?\"\n",
       "3. \"Let me look into this for you.\"\n",
       "4.\"Our policy is as follows...\"\n",
       "5.\"Would you like me to escalate this issue?\"\n",
       "6.'\"We apologize, but [specific reason].\"\n",
       "7.\"'s the status of your order?\n",
       "8.\"\"How did we do today? Is there anything else I can help with?\"\n",
       "9. \"I'm going to need some more information from you.\"\n",
       "10. \"Our team will get back to you within 24 hours.\"\n",
       "11.\"Is this an issue or a question?\"\n",
       "12.\"'s the next step in resolving your concern?\n",
       "13.\"\"We're here for our valued customers...\"\n",
       "14.\"\"How can I make it right for you today?\"\n",
       "15. \"\"Let me check on that availability\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 4.3: High frequency penalty (strong reduction in repetition, might affect coherence)\n",
    "generate_and_display(\n",
    "    common_prompt_freq_penalty,\n",
    "    \"frequency_penalty\", 2.0, # Max penalty for strong effect\n",
    "    temperature=0.7,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Presence_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Encouraging a customer service bot to explore different solutions/topics, generating comprehensive meeting minutes by covering all points, producing diverse content ideas for a campaign.\n",
    "\n",
    "**What to Observe**:\n",
    "\n",
    "- **No** penalty (0.0): The model might dwell on a few initial topics or points without moving on to others mentioned in the conceptual prompt.\n",
    "\n",
    "- **Moderate** penalty: The summary should cover a broader range of distinct action items or concepts, rather than just elaborating on the first few.\n",
    "\n",
    "- **High** penalty: The model will try hard to introduce new concepts or distinct ideas, possibly jumping between points quickly or even generating somewhat disjointed output if forced too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_pres_penalty = \"\"\"\n",
    "You are leading a quick internal meeting discussing how Generative AI can significantly improve our current sales and customer engagement processes. \n",
    "Briefly outline 4 distinct areas or strategies where GenAI could have a major impact, providing a brief example for each.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating presence_penalty=0.0 ---\n",
      "Prompt: \"\n",
      "You are leading a quick internal meeting discussing how Generative AI can significantly improve our current sales and customer engagement processes. \n",
      "Briefly outline 4 distinct areas or strategies where GenAI could have a major impact, providing a brief example for each.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Good morning team,\n",
       "\n",
       "As we explore the potential of Generative AI to enhance our sales and customer engagement processes, I'd like to highlight four key areas where this technology can make a significant impact. Here are some examples:\n",
       "\n",
       "**1. Personalized Content Creation**\n",
       "\n",
       "GenAI can help us generate personalized content, such as product descriptions, customer testimonials, or even entire marketing campaigns. For instance, we could use GenAI to create customized email templates that address individual customers' interests and needs.\n",
       "\n",
       "Example: A customer's name is inserted into a template, and the AI generates a unique email with relevant product recommendations based on their purchase history.\n",
       "\n",
       "**2. Automated Sales Conversations**\n",
       "\n",
       "GenAI-powered chatbots can engage customers in real-time, providing personalized support and answering common questions. This can help reduce response times and improve overall customer satisfaction.\n",
       "\n",
       "Example: A customer asks about a product's features, and the GenAI-powered chatbot responds with relevant information, highlighting key benefits and answering questions.\n",
       "\n",
       "**3. Data-Driven Lead Scoring**\n",
       "\n",
       "GenAI can analyze vast amounts of customer data to predict lead behavior and score them accordingly. This allows us to prioritize leads and focus on high-potential customers.\n",
       "\n",
       "Example: A GenAI algorithm analyzes a large dataset of customer interactions, identifying patterns and predicting the likelihood of conversion for each lead.\n",
       "\n",
       "**4. Dynamic Content Generation**\n",
       "\n",
       "GenAI can create dynamic content that adapts to changing market conditions or customer preferences. This enables us to respond quickly to emerging trends and offer highly relevant offers.\n",
       "\n",
       "Example: A GenAI-powered system analyzes real-time data on a popular competitor's pricing strategy and generates a counteroffer with tailored discounts for our customers, increasing the likelihood of conversion.\n",
       "\n",
       "These are just a few examples of how Generative AI can enhance our sales and customer engagement processes. Let's discuss these ideas further and explore ways to integrate them into our operations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 5.1: No presence penalty (might focus heavily on one or two main themes)\n",
    "generate_and_display(\n",
    "    common_prompt_pres_penalty,\n",
    "    \"presence_penalty\", 0.0,\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating presence_penalty=1.0 ---\n",
      "Prompt: \"\n",
      "You are leading a quick internal meeting discussing how Generative AI can significantly improve our current sales and customer engagement processes. \n",
      "Briefly outline 4 distinct areas or strategies where GenAI could have a major impact, providing a brief example for each.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Good morning team,\n",
       "\n",
       "As we explore the possibilities of Generative AI in our business, I'd like to highlight four key areas where this technology can make a significant impact:\n",
       "\n",
       "**Area 1: Personalized Sales Content Generation**\n",
       "\n",
       "With GenAI, we can generate customized sales content, such as product descriptions, social media posts, and marketing emails. For example, an AI-powered tool could analyze customer data and preferences to create personalized product recommendations for each individual.\n",
       "\n",
       "* Example: A customer has browsed multiple products on our website, but hasn't made a purchase yet. Our GenAI system generates a targeted email with recommended products tailored to their interests.\n",
       "* Potential impact: Increased sales conversion rates by up to 25%\n",
       "\n",
       "**Area 2: Chatbot-Powered Customer Engagement**\n",
       "\n",
       "GenAI can help create more effective chatbots that provide personalized customer support and responses. This enables us to offer 24/7 engagement, reduce response times, and improve overall customer satisfaction.\n",
       "\n",
       "* Example: A customer contacts our chatbot with a query about their order status. The GenAI system responds promptly with accurate information and helpful next steps.\n",
       "* Potential impact: Reduced customer care costs by up to 30% through reduced support requests\n",
       "\n",
       "**Area 3: Dynamic Content Generation for Marketing Campaigns**\n",
       "\n",
       "GenAI can help generate dynamic content, such as images, videos, and social media posts, that adapt to changing market conditions and audience interests. This enables us to create more engaging marketing campaigns that resonate with our target audience.\n",
       "\n",
       "* Example: During a product launch, our GenAI system generates eye-catching visuals showcasing the new product features.\n",
       "* Potential impact: Increased brand visibility by up to 40% through improved engagement metrics\n",
       "\n",
       "**Area 4: Predictive Lead Scoring and Qualification**\n",
       "\n",
       "GenAI can help analyze customer data to predict lead behavior, identify potential issues, and qualify leads more accurately. This enables us to focus on high-quality leads that are more likely to convert into sales.\n",
       "\n",
       "* Example: An AI-powered system analyzes a new lead's browsing history, purchase intent, and demographic information to determine their likelihood of converting.\n",
       "* Potential impact: Reduced lead-to-opportunity conversion rates by up to 20% through targeted resource allocation\n",
       "\n",
       "These examples illustrate just a few possible ways GenAI can enhance our sales and customer engagement processes. I believe that with careful implementation, we can unlock significant value from this technology.\n",
       "\n",
       "Next steps?\n",
       "\n",
       "Would anyone like to discuss potential implementation strategies or challenges in more detail?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 5.2: Moderate presence penalty (encourages covering a broader range of distinct action items)\n",
    "generate_and_display(\n",
    "    common_prompt_pres_penalty,\n",
    "    \"presence_penalty\", 1.0, # Increased for more impact\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating presence_penalty=2.0 ---\n",
      "Prompt: \"\n",
      "You are leading a quick internal meeting discussing how Generative AI can significantly improve our current sales and customer engagement processes. \n",
      "Briefly outline 4 distinct areas or strategies where GenAI could have a major impact, providing a brief example for each.\n",
      "\"\n",
      "Other consistent parameters: {'temperature': 0.7}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Thank you everyone! Today, we'll explore the vast potential of Generative AI to enhance our sales and customer engagement processes.\n",
       "\n",
       "Here are four key areas where GenAI can make significant improvements:\n",
       "\n",
       "**1. Personalized Sales Content Generation**\n",
       "GenAI-powered tools can analyze market trends, customer data, and industry insights to create tailored sales content at scale. For example:\n",
       "\t* Our marketing team uses a GenAI platform to generate product descriptions that highlight unique selling points for each new device release.\n",
       "\t* The AI-generated copy improves conversion rates by 15% compared to traditional manual writing.\n",
       "\n",
       "**2. Chatbot-Powered Customer Support**\n",
       "GenAIs can drive more efficient and empathetic customer support experiences through chatbots, reducing response times and improving resolution rates. For instance:\n",
       "\t* Our company deploys a GenAI-powered chatbot that helps customers with basic queries, freeing up human support agents to focus on complex issues.\n",
       "\t* The chatbot's accuracy improves by 22% compared to traditional scripted responses.\n",
       "\n",
       "**3. Predictive Lead Scoring**\n",
       "GenAIs can analyze large datasets to identify high-potential leads and predict sales success rates more accurately than manual evaluation methods. For example:\n",
       "\t* We integrate a GenAI-powered lead scoring system that evaluates customer behavior, preferences, and purchase history.\n",
       "\t* The AI scores lead quality with an accuracy of 92%, enabling our sales teams to prioritize their efforts effectively.\n",
       "\n",
       "**4. Content Recommendation Engine**\n",
       "GenAIs can use natural language processing (NLP) to analyze vast amounts of content data, providing personalized recommendations for customers based on their interests and preferences. For instance:\n",
       "\t* Our company develops a GenAI-powered content recommendation engine that suggests relevant products or services to our users.\n",
       "\t* The user engagement increases by 25% after implementing the AI-driven recommendation system.\n",
       "\n",
       "These examples demonstrate how Generative AI can revolutionize various aspects of our sales and customer engagement processes, driving efficiency, accuracy, and improved outcomes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 5.3: High presence penalty (strong encouragement for new ideas, potentially disjointed)\n",
    "generate_and_display(\n",
    "    common_prompt_pres_penalty,\n",
    "    \"presence_penalty\", 2.0, # Max penalty for strong effect\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Ensuring chatbot responses don't continue past a natural break, extracting specific data blocks (e.g., JSON), generating bulleted lists that don't overextend, completing fill-in-the-blank forms.\n",
    "garding:\"\n",
    "\n",
    "**What to Observe**: The model's generation will immediately halt once any of the provided stop sequences appear in its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating stop=['\\n'] ---\n",
      "Prompt: \"\n",
      "Draft a quick customer service response template:\n",
      "Thank you for contacting TrueFoundry. We received your inquiry regarding:\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a quick customer service response template:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 6.1: Stopping at newline (for single-line completions)\n",
    "common_prompt_stop = \"\"\"\n",
    "Draft a quick customer service response template:\\nThank you for contacting TrueFoundry. We received your inquiry regarding:\"\"\"\n",
    "generate_and_display(\n",
    "    common_prompt_stop,\n",
    "    \"stop\", [\"\\n\"],\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating stop=['Best regards,'] ---\n",
      "Prompt: \"Draft a professional email to a client regarding their contract status. \n",
      "Subject: Your Contract #12345 Update\n",
      "Dear [Client Name],\n",
      "\n",
      "Your recent contract #12345 has been:\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a draft of the email:\n",
       "\n",
       "Subject: Your Contract #12345 Update\n",
       "\n",
       "Dear [Client Name],\n",
       "\n",
       "I am writing to provide you with an update on the status of your contract, #12345. As we previously discussed, our team is actively working to complete the necessary tasks and ensure that all requirements are met.\n",
       "\n",
       "As of today, I am pleased to inform you that:\n",
       "\n",
       "[Insert one of the following options]\n",
       "\n",
       "* The contract has been finalized and is ready for signature.\n",
       "* There have been some minor revisions required, which will be communicated to you separately.\n",
       "* Additional information or clarification is needed from your end before we can proceed with the next steps.\n",
       "\n",
       "Please let me know if you have any questions or concerns regarding this update. I am more than happy to provide further details and keep you informed of our progress.\n",
       "\n",
       "If you would like to review the contract, please find it attached to this email for your convenience. If you require any assistance or need to schedule a call to discuss, please don't hesitate to reach out.\n",
       "\n",
       "Thank you for your patience and cooperation. I look forward to hearing from you soon."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 6.2: Stopping at a specific closing phrase (for structured email generation)\n",
    "common_prompt_email = \"\"\"Draft a professional email to a client regarding their contract status. \n",
    "Subject: Your Contract #12345 Update\\nDear [Client Name],\\n\\nYour recent contract #12345 has been:\"\"\"\n",
    "generate_and_display(\n",
    "    common_prompt_email,\n",
    "    \"stop\", [\"Best regards,\"], # Model will stop before adding the closing\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating stop=['4.'] ---\n",
      "Prompt: \"List 3 benefits of our AI gateway:\n",
      "1. AI Governance\n",
      "2. Efficient budgeting\n",
      "3.\"\n",
      "Other consistent parameters: {'temperature': 0.5}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here is the completed list with the third benefit:\n",
       "\n",
       "3. Enhanced decision-making: Our AI gateway provides real-time insights and analytics, enabling data-driven decisions that can drive business growth and optimization.\n",
       "\n",
       "So, to recap, the three benefits of our AI gateway are:\n",
       "\n",
       "1. **AI Governance**: Ensures the responsible development, deployment, and use of AI within the organization.\n",
       "2. **Efficient budgeting**: Automates budget allocation and tracking, reducing errors and improving financial management.\n",
       "3. **Enhanced decision-making**: Provides real-time insights and analytics to inform business decisions and drive growth."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 6.3: Stopping after a list item (for controlled list generation)\n",
    "common_prompt_list = \"List 3 benefits of our AI gateway:\\n1. AI Governance\\n2. Efficient budgeting\\n3.\"\n",
    "generate_and_display(\n",
    "    common_prompt_list,\n",
    "    \"stop\", [\"4.\"], # Stop before generating the 4th item (assuming it would start with 4.)\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commercial Use Cases**: Reproducible content for A/B testing (ensuring differences are due to prompt, not randomness), consistent chatbot behavior for quality assurance, debugging prompt engineering, generating consistent test data.\n",
    "\n",
    "**What to Observe**:\n",
    "\n",
    "- **Same** seed: The outputs for the multiple runs should be identical or extremely close.\n",
    "- **Different** seed: The outputs, while still relevant to the prompt, should be distinct from each other across different seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt_seed = \"Generate a marketing slogan for a new eco-friendly cleaning product. Only return the slogan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing reproducibility with the SAME seed (42) ---\n",
      "\n",
      "--- Demonstrating seed=42 ---\n",
      "Prompt: \"Generate a marketing slogan for a new eco-friendly cleaning product. Only return the slogan.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 20}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Clean with a Clear Conscience, Naturally.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Clean with a Clear Conscience, Naturally.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Clean with a Clear Conscience, Naturally.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 7.1: Same seed, same prompt, same parameters (for reproducible slogan generation for testing)\n",
    "print(\"\\n--- Testing reproducibility with the SAME seed (42) ---\")\n",
    "generate_and_display(\n",
    "    common_prompt_seed,\n",
    "    \"seed\", 42,\n",
    "    num_runs=3, # Run multiple times to show consistency\n",
    "    temperature=0.7,\n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing different seeds for DIVERSE slogan options ---\n",
      "\n",
      "--- Demonstrating seed=101 ---\n",
      "Prompt: \"Generate a marketing slogan for a new eco-friendly cleaning product. Only return the slogan.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 20}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Sparkle with a Clear Conscience, Clean with Green.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 7.2: Different seeds, same prompt, same parameters (to get diverse options for review)\n",
    "print(\"\\n--- Testing different seeds for DIVERSE slogan options ---\")\n",
    "generate_and_display(\n",
    "    common_prompt_seed,\n",
    "    \"seed\", 101,\n",
    "    num_runs=1, # One run for each distinct seed is enough to show variety\n",
    "    temperature=0.7,\n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating seed=202 ---\n",
      "Prompt: \"Generate a marketing slogan for a new eco-friendly cleaning product. Only return the slogan.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 20}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Clean with a Clear Conscience, Inside and Out.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 7.3: Different seeds, same prompt, same parameters (to get diverse options for review)\n",
    "generate_and_display(\n",
    "    common_prompt_seed,\n",
    "    \"seed\", 202,\n",
    "    num_runs=1,\n",
    "    temperature=0.7,\n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating seed=303 ---\n",
      "Prompt: \"Generate a marketing slogan for a new eco-friendly cleaning product. Only return the slogan.\"\n",
      "Other consistent parameters: {'temperature': 0.7, 'max_tokens': 20}\n",
      "\n",
      "--- Run 1 ---\n",
      "Generated Text:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Clean with a Clear Conscience, Naturally.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Demonstration for this parameter setting ---\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 7.4: Different seeds, same prompt, same parameters (to get diverse options for review)\n",
    "generate_and_display(\n",
    "    common_prompt_seed,\n",
    "    \"seed\", 303,\n",
    "    num_runs=1,\n",
    "    temperature=0.7,\n",
    "    max_tokens=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
