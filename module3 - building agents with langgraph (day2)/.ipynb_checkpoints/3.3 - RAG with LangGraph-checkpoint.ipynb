{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "131e14d0",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30632154",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10fca11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, getpass, warnings\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce25dbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b13fa3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Groq API Key\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"GROQ API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b191e9",
   "metadata": {},
   "source": [
    "## Defining Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191901d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question\n",
    "question = \"What is TrueFoundry all about?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec991ab",
   "metadata": {},
   "source": [
    "### Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e79ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model_name = \"llama-3.1-8b-instant\"\n",
    "llm = init_chat_model(model_name, model_provider=\"groq\") #Other Llama alternatives available are llama3-8b-8192, llama-3.3-70b-versatile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d51397",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6d5af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 667b0c1932bc: 100% ▕██████████████████▏ 4.9 GB                         \u001b[K\n",
      "pulling 948af2743fc7: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 0ba8f0e314b4: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 455f34728c9b: 100% ▕██████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe886d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings_model = OllamaEmbeddings(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e527e0",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a544c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ef1bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a vector store\n",
    "vector_store_chroma = Chroma(\n",
    "    collection_name=\"rag_with_langgraph\",\n",
    "    embedding_function=embeddings_model,\n",
    "    # persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91794b05",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df8599",
   "metadata": {},
   "source": [
    "### Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02c1e2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4 #import Beautiful Soup\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146c2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the main content from the full HTML\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"section_gateway-banner ai\", \n",
    "                                        \"section_traces-features ai_gateway\",\n",
    "                                       \"section_home-ai\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.truefoundry.com/ai-gateway\", \n",
    "               \"https://www.truefoundry.com\"),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4684cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Post-processing to clean up excessive newlines and whitespace ---\n",
    "for i, doc in enumerate(docs):\n",
    "    if doc.page_content:\n",
    "        # 1. Replace multiple newlines with a single newline\n",
    "        cleaned_content = re.sub(r'\\n\\s*\\n', '\\n\\n', doc.page_content)\n",
    "        # 2. Replace multiple spaces with a single space\n",
    "        cleaned_content = re.sub(r' {2,}', ' ', cleaned_content)\n",
    "        # 3. Strip leading/trailing whitespace from each line\n",
    "        cleaned_content = '\\n'.join([line.strip() for line in cleaned_content.split('\\n')])\n",
    "        # 4. Remove leading/trailing whitespace from the whole string\n",
    "        cleaned_content = cleaned_content.strip()\n",
    "    \n",
    "        docs[i].page_content = cleaned_content\n",
    "# --- End of post-processing ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0074ee",
   "metadata": {},
   "source": [
    "### Splitting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5328d1c7-0083-4a7b-8b42-cb356e2de02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a83c157-925a-4c59-8d0b-4892596fd5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text splitter\n",
    "text_splitter_semantic = SemanticChunker(embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a70ee779-b59d-430a-8eaf-6de2f9079d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "647"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaf8038a-67e5-4d76-95d5-43a092f5844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce content of second doc for faster runtimes\n",
    "from langchain_core.documents import Document\n",
    "docs_truncated = docs.copy()\n",
    "docs_truncated[1] = Document(page_content= docs[1].page_content[:5000], metadata=docs[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b298d532-fcc2-42ec-a30b-8fbd3b2bb71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split input into 4 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "#Split the documents into chunks (only pick second one)\n",
    "all_splits_semantic = text_splitter_semantic.split_documents(docs_truncated)\n",
    "print(f\"Split input into {len(all_splits_semantic)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29371bbc-80d8-4a98-9a50-c9c6b0f26929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Split0 ---\n",
      "\n",
      "Secure Deployment : VPC | On-Prem | Air-GappedAI Gateway: Fast, Scalable, Enterprise-ReadyEnterprise-Ready AI Gateway for secure, high-performance LLM access, observability, and orchestration.See detailed pricingGet Started for FreeAI Gateway: Unified LLM API AccessSimplify your GenAI stack with a single AI Gateway that integrates all major models. Connect to OpenAI, Claude, Gemini, Groq, Mistral, and 250+ LLMs through one AI Gateway API\n",
      "\n",
      "Use the platform to support chat, completion, embedding, and reranking model types. Centralize API key management and team authentication in one place. Orchestrate multi-model workloads seamlessly through your infrastructure.Read MoreAI Gateway Observability\n",
      "\n",
      "Monitor token usage, latency, error rates, and request volumes across your system. Store and inspect full request/response logs centrally to ensure compliance and simplify debugging. Tag traffic with metadata like user ID, team, or environment to gain granular insights. Filter logs and metrics by model, team, or geography to quickly pinpoint root causes and accelerate resolution.Read MoreQuota & Access Control via AI GatewayEnforce governance, control costs, and reduce risk with consistent policy management. Apply rate limits per user, service, or endpoint. Set cost-based or token-based quotas using metadata filters.\n",
      "\n",
      "--- Split1 ---\n",
      "\n",
      "Use role-based access control (RBAC) to isolate and manage usage. Govern service accounts and agent workloads at scale through centralized rules.Read MoreEnsuring predictable usage, strong access boundaries, and scalable team-level governance for your GenAI infrastructure.Low-Latency Inference Run your most performance-sensitive workloads through a high-speed infrastructure. Achieve sub-3ms internal latency even under enterprise-scale workloads. Scale seamlessly to manage burst traffic and high-throughput workloads. Deliver predictable response times for real-time chat, RAG, and AI assistants. Place deployments close to inference layers to minimize latency and eliminate network lag.Read MorePlace the AI Gateway directly in your production inference path — its low-latency architecture ensures no performance tradeoffs.AI Gateway Routing & FallbacksEnsure reliability, even during model failures, with smart AI Gateway traffic controls. Supports latency-based routing to the fastest available LLM. Distribute traffic intelligently using weighted load balancing for reliability and scale. Automatically fallback to secondary models when a request fails. Use geo-aware routing to meet regional compliance and availability needs.Read MoreThis system ensures you never go offline, even when individual models face downtime or spike in latency.Serve Self-Hosted Models Expose open-source models with full control. Deploy LLaMA, Mistral, Falcon, and more with zero SDK changes. Full compatibility with vLLM, SGLang, KServe, and Triton. Streamline operations with Helm-based management of autoscaling, GPU scheduling, and deployments\n",
      "\n",
      "Run your own models in VPC, hybrid, or air-gapped environments.Read MoreAI Gateway + MCP IntegrationPower secure agent workflows through the AI Gateway’s native MCP support. Connect enterprise tools like Slack, GitHub, Confluence, and Datadog.\n",
      "\n",
      "--- Split2 ---\n",
      "\n",
      "Easily register internal MCP Servers with minimal setup required. Apply OAuth2, RBAC, and metadata policies to every tool call.Read MoreAI Gateway Guardrails\n",
      "\n",
      "Seamlessly enforce your own safety guardrails, including PII filtering and toxicity detection\n",
      "\n",
      "Customize the AI Gateway with guardrails tailored to your compliance and safety needsRead More\n",
      "\n",
      "--- Split3 ---\n",
      "\n",
      "Orchestrate Agentic AI with AI GatewayEnable intelligent multi-step reasoning, tool usage, and memory with full control and visibility across your AI agents and workflows.AI GatewayManage agent memory, tool orchestration, and action planning through a centralized protocol that supports complex, context-aware workflows.Learn More\n",
      "\n",
      "MCP & Agents RegistryMaintain a structured, discoverable registry of tools and APIs accessible to agents, complete with schema validation and access control.Learn More\n",
      "\n",
      "Prompt Lifecycle ManagementVersion, manage, and monitor prompts to ensure high-quality, repeatable behavior across agents and use cases.Learn More\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_splits_semantic)):\n",
    "    print(f\"\\n--- Split{i} ---\\n\")\n",
    "    print(all_splits_semantic[i].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e070a",
   "metadata": {},
   "source": [
    "### Storing in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b50f7fd-e35b-481e-9585-3a649a950b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0c288e38-b127-40fc-9ed6-90681aef2964',\n",
       " '8786ce46-226e-4b4f-8f79-47a809d66d6a',\n",
       " 'e51d8ce0-d060-4dca-ba4c-49debeff217d',\n",
       " 'e63ba32e-98ae-44b1-a927-c0e2d56823b6']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add all chunks to vector DB\n",
    "uuids = [str(uuid4()) for _ in range(len(all_splits_semantic))] #Universally unique identifier\n",
    "vector_store_chroma.add_documents(documents=all_splits_semantic, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e558d1f",
   "metadata": {},
   "source": [
    "## Retrieval and Generation with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64dff69",
   "metadata": {},
   "source": [
    "### Define Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d6374cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "#Use Prompt Template\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0435516",
   "metadata": {},
   "source": [
    "### Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aca81c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5691a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store_chroma.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77fa28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph, END\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_node(\"generate\", generate)\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\",\"generate\")\n",
    "graph_builder.add_edge(\"generate\",END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# # Add memory\n",
    "# memory = MemorySaver()\n",
    "# graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76a39287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.graph import START, StateGraph\n",
    "\n",
    "# graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "# graph_builder.add_edge(START, \"retrieve\")\n",
    "# graph = graph_builder.compile()\n",
    "\n",
    "# # # Add memory\n",
    "# # memory = MemorySaver()\n",
    "# # graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# # config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb33aedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5c3af",
   "metadata": {},
   "source": [
    "### Invoke Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e13ec649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='e63ba32e-98ae-44b1-a927-c0e2d56823b6', metadata={'source': 'https://www.truefoundry.com'}, page_content='Orchestrate Agentic AI with AI GatewayEnable intelligent multi-step reasoning, tool usage, and memory with full control and visibility across your AI agents and workflows.AI GatewayManage agent memory, tool orchestration, and action planning through a centralized protocol that supports complex, context-aware workflows.Learn More\\n\\nMCP & Agents RegistryMaintain a structured, discoverable registry of tools and APIs accessible to agents, complete with schema validation and access control.Learn More\\n\\nPrompt Lifecycle ManagementVersion, manage, and monitor prompts to ensure high-quality, repeatable behavior across agents and use cases.Learn More'), Document(id='8786ce46-226e-4b4f-8f79-47a809d66d6a', metadata={'source': 'https://www.truefoundry.com/ai-gateway'}, page_content='Use role-based access control (RBAC) to isolate and manage usage. Govern service accounts and agent workloads at scale through centralized rules.Read MoreEnsuring predictable usage, strong access boundaries, and scalable team-level governance for your GenAI infrastructure.Low-Latency Inference\\xa0Run your most performance-sensitive workloads through a high-speed infrastructure. Achieve sub-3ms internal latency even under enterprise-scale workloads. Scale seamlessly to manage burst traffic and high-throughput workloads. Deliver predictable response times for real-time chat, RAG, and AI assistants. Place deployments close to inference layers to minimize latency and eliminate network lag.Read MorePlace the AI Gateway directly in your production inference path — its low-latency architecture ensures no performance tradeoffs.AI Gateway Routing & FallbacksEnsure reliability, even during model failures, with smart AI Gateway traffic controls. Supports latency-based routing to the fastest available LLM. Distribute traffic intelligently using weighted load balancing for reliability and scale. Automatically fallback to secondary models when a request fails. Use geo-aware routing to meet regional compliance and availability needs.Read MoreThis system ensures you never go offline, even when individual models face downtime or spike in latency.Serve Self-Hosted Models Expose open-source models with full control. Deploy LLaMA, Mistral, Falcon, and more with zero SDK changes. Full compatibility with vLLM, SGLang, KServe, and Triton. Streamline operations with Helm-based management of autoscaling, GPU scheduling, and deployments\\n\\nRun your own models in VPC, hybrid, or air-gapped environments.Read MoreAI Gateway + MCP IntegrationPower secure agent workflows through the AI Gateway’s native MCP support. Connect enterprise tools like Slack, GitHub, Confluence, and Datadog.'), Document(id='e51d8ce0-d060-4dca-ba4c-49debeff217d', metadata={'source': 'https://www.truefoundry.com/ai-gateway'}, page_content='Easily register internal MCP Servers with minimal setup required. Apply OAuth2, RBAC, and metadata policies to every tool call.Read MoreAI Gateway Guardrails\\n\\nSeamlessly enforce your own safety guardrails, including PII filtering and toxicity detection\\n\\nCustomize the AI Gateway with guardrails tailored to your compliance and safety needsRead More'), Document(id='0c288e38-b127-40fc-9ed6-90681aef2964', metadata={'source': 'https://www.truefoundry.com/ai-gateway'}, page_content='Secure Deployment : VPC | On-Prem | Air-GappedAI Gateway: Fast, Scalable, Enterprise-ReadyEnterprise-Ready AI Gateway for secure, high-performance LLM access, observability, and orchestration.See detailed pricingGet Started for FreeAI Gateway: Unified LLM API AccessSimplify your GenAI stack with a single AI Gateway that integrates all major models. Connect to OpenAI, Claude, Gemini, Groq, Mistral, and 250+ LLMs through one AI Gateway API\\n\\nUse the platform to support chat, completion, embedding, and reranking model types. Centralize API key management and team authentication in one place. Orchestrate multi-model workloads seamlessly through your infrastructure.Read MoreAI Gateway Observability\\n\\nMonitor token usage, latency, error rates, and request volumes across your system. Store and inspect full request/response logs centrally to ensure compliance and simplify debugging. Tag traffic with metadata like user ID, team, or environment to gain granular insights. Filter logs and metrics by model, team, or geography to quickly pinpoint root causes and accelerate resolution.Read MoreQuota & Access Control via AI GatewayEnforce governance, control costs, and reduce risk with consistent policy management. Apply rate limits per user, service, or endpoint. Set cost-based or token-based quotas using metadata filters.')]\n",
      "\n",
      "\n",
      "Answer: TrueFoundry is an AI infrastructure platform that enables orchestration of Agentic AI through its AI Gateway, which manages agent memory, tool orchestration, and action planning. The platform provides features such as role-based access control, low-latency inference, and secure deployment options. It also supports integration with various enterprise tools and models.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": question})\n",
    "\n",
    "## ----- To run with memory -----\n",
    "# result = graph.invoke({\"question\": question},config)\n",
    "\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer: {result[\"answer\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
