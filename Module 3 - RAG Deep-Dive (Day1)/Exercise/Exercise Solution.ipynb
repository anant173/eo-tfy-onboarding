{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a98b70a-bf92-4ff7-9abb-4cf373df37ba",
   "metadata": {},
   "source": [
    "# Smart Grocery Cart Assistant  \n",
    "*Built on top of the JioMart Retail Product Catalog, [sourced](https://www.kaggle.com/datasets/satyamsundaram/jiomart-products-dataset) from Kaggle*\n",
    "\n",
    "---\n",
    "\n",
    "## Objective  \n",
    "Create a Gradio-based AI assistant that recommends a weekly grocery shopping cart based on the user's dietary needs and preferences. The app uses Retrieval-Augmented Generation (RAG) on a structured JioMart product dataset, generating explainable, structured results that are visually rendered like a real shopping cart.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1 — User App (Essential Features)\n",
    "\n",
    "### Inputs\n",
    "- **Grocery Needs**: Free-form text input  \n",
    "  *(e.g., “high protein, no besan or curd”)*\n",
    "  \n",
    "### Output\n",
    "- **Suggestion**: A natural language explanation of what's recommended and why\n",
    "- **Shopping Cart**: Structured as a visual gallery containing:\n",
    "  - Product Name\n",
    "  - Quantity\n",
    "  - Price\n",
    "  - Product Image\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2 — Visual Experience\n",
    "\n",
    "### Visual Cart (Gradio `gr.Gallery`)\n",
    "The cart is rendered in a grid format using `gr.Gallery`, where each item includes:\n",
    "- Product image (`image_url`)\n",
    "- Caption text with:\n",
    "  - `item_name`\n",
    "  - `Qty.{quantity}`\n",
    "  - `₹total_price`\n",
    "\n",
    "This provides a realistic and user-friendly shopping experience.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3 — Developer-Facing Advanced Settings\n",
    "\n",
    "Shown under a collapsible **LLM Settings (Advanced)** section in the UI.\n",
    "\n",
    "### Model & Generation Settings\n",
    "- **Model Selector**:\n",
    "  - GPT-4o-Mini (`gpt-4o-mini`)\n",
    "  - LLaMA 3.3 70B (`llama-3.3-70b-versatile`)\n",
    "- **Temperature Slider**: Adjustable between 0.0 and 1.5\n",
    "\n",
    "These settings allow developers to tune how deterministic or creative the model’s responses are.\n",
    "\n",
    "---\n",
    "\n",
    "## Backend Setup\n",
    "\n",
    "### Vector Store and RAG\n",
    "- **Embedding Model**: LLaMA 3.2 (3B) via Ollama\n",
    "- **Vector Database**: ChromaDB\n",
    "- **RAG Workflow**:\n",
    "  1. Load product catalog using `CSVLoader`\n",
    "  2. Parse and clean fields into `Document` objects with metadata\n",
    "  3. Generate embeddings and index documents\n",
    "  4. Perform similarity search based on user preferences\n",
    "  5. Use an LLM to generate structured output parsed via `PydanticOutputParser`\n",
    "\n",
    "---\n",
    "\n",
    "## Sample CSV Schema Mapping\n",
    "\n",
    "| Field in CSV     | Mapped Use              |\n",
    "|------------------|-------------------------|\n",
    "| `title`          | `item_name` and `quantity` (parsed from name) |\n",
    "| `discountedPrice`| `unit_price`            |\n",
    "| `filename`       | `image_url`             |\n",
    "| `subType`        | `sub_category`          |\n",
    "| `type`           | `category`              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c02ab-a33d-4a2b-a695-16a0af6f54dd",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e6a209-d88b-4c2a-a524-4d0765035c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install langchain_google_vertexai langchain_deepseek langchain_anthropic langchain_openai langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842b9320-c1bc-4015-a798-cf9b2d6ed83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema.output_parser import OutputParserException\n",
    "from langchain_core.documents import Document\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37a92f3-a819-4f8b-8d7c-8e50d0fba21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3394048e-92aa-4efe-a065-17862a827072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for API Keys\n",
    "api_keys = [\"OPENAI_API_KEY\",\"GROQ_API_KEY\"]\n",
    "for api_key in api_keys:\n",
    "    if api_key not in os.environ:\n",
    "        os.environ[api_key] = getpass.getpass(\"Enter the API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90c9ca-8796-4d48-8955-56207ad10c58",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10883c2c-f74f-4d2e-9e8e-4bfc39ceaf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "loader = CSVLoader(file_path=\"jiomart_products_database.csv\", source_column=\"title\")\n",
    "documents_raw = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c772180-6fcb-4b17-bbb0-47a2d6730f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert page_content string to dict and build metadata\n",
    "documents = []\n",
    "for doc in documents_raw:\n",
    "    try:\n",
    "        row_data = dict(\n",
    "            line.split(\":\", 1) for line in doc.page_content.split(\"\\n\") if \":\" in line\n",
    "        )\n",
    "        row_data = {k.strip(): v.strip() for k, v in row_data.items()}\n",
    "\n",
    "        page_text = f\"Name: {row_data.get('title', '')} | Sub-type: {row_data.get('subType', '')} | Type: {row_data.get('type', '')} | Price: {row_data.get('discountedPrice', 0)} | Image: {row_data.get('filename', '')}\"\n",
    "        metadata = {\n",
    "            \"category\": row_data.get(\"type\", \"\"),\n",
    "            \"sub_category\": row_data.get(\"subType\", \"\")\n",
    "        }\n",
    "\n",
    "        documents.append(Document(page_content=page_text, metadata=metadata))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Skipping row due to error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53920bc5-520c-42bb-9d99-a5381c7b752b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5672"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a4ae5-a9e9-432c-874b-843dade789d1",
   "metadata": {},
   "source": [
    "## Storing in Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325c79ba-d931-467e-b691-31c812ff3fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 74701a8c35f6: 100% ▕██████████████████▏ 1.3 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 4f659a1e86d7: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "!ollama pull llama3.2:1b\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04577f01-3601-4c81-bfae-b8e3fe7e4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in Chroma - 100 rows only for quicker processing\n",
    "vectorstore = Chroma.from_documents(documents[:100], embedding_model)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841db84d-05cb-4ed2-ae42-8048c893b579",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517f27c0-6733-46bf-9eda-3ac1d4d66733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic schema\n",
    "class GroceryItem(BaseModel):\n",
    "    item_name: str\n",
    "    price: float\n",
    "    quantity: int\n",
    "    image_url: str\n",
    "\n",
    "class GroceryOutput(BaseModel):\n",
    "    reasoning: str\n",
    "    items: list[GroceryItem]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=GroceryOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baad3961-d934-4464-8ceb-814608c18293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "template = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful grocery assistant. Based on the user preferences below and the product catalog context, recommend a short list of minimum 10 items for a weekly diet intake. \n",
    "Make sure items are not repeated in the output.\n",
    "\n",
    "User Preferences: {preferences}\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Return your response as a JSON following this format:\n",
    "{format_instructions}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97385f6-86c9-42d0-adc3-246532528233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model name map\n",
    "model_name_map = {\n",
    "    \"GPT-4o-Mini\": \"gpt-4o-mini\",\n",
    "    \"Llama-3.3\": \"llama-3.3-70b-versatile\"\n",
    "}\n",
    "\n",
    "model_choices = [\n",
    "    \"GPT-4o-Mini (OpenAI)\",\n",
    "    \"Llama-3.3 (70B) (Groq)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "decfeaf2-9bcc-41b8-8880-98b12caf61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selector\n",
    "def get_llm(model_choice, temperature):\n",
    "    if \"Groq\" in model_choice:\n",
    "        return init_chat_model(model_name_map[model_choice.split(\" \")[0]], model_provider=\"groq\", temperature = temperature)\n",
    "    elif \"OpenAI\" in model_choice:\n",
    "        return init_chat_model(model_name_map[model_choice.split(\" \")[0]], model_provider=\"openai\", temperature = temperature)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6737ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing RAG pipeline line-by-line\n",
    "preference = \"I want a high fat diet\"\n",
    "context_docs = retriever.invoke(preference) #Retrieving relevant food items\n",
    "relevant_text = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "llm = get_llm(model_choices[0], 0)\n",
    "chain = template | llm | parser\n",
    "input_dict = {\n",
    "    \"preferences\": preference,\n",
    "    \"context\": relevant_text,\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "}\n",
    "chain.invoke(input_dict).model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4018e-e25d-4213-bc3f-ce0ca3a278a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG pipeline\n",
    "def generate_cart(model_choice, user_input):\n",
    "    context_docs = retriever.invoke(user_input[\"preferences\"]) #Retrieving relevant food items\n",
    "    relevant_text = \"\\n\".join([doc.page_content for doc in context_docs]) #Joining the food items\n",
    "    llm = get_llm(user_input[\"model_choice\"], user_input[\"temperature\"])\n",
    "    \n",
    "    # #Invoking - Option 1\n",
    "    # prompt = template.format(\n",
    "    #     preferences=user_input[\"preferences\"],\n",
    "    #     context=relevant_text,\n",
    "    #     format_instructions=parser.get_format_instructions()\n",
    "    # )\n",
    "    # output = llm.invoke(prompt)\n",
    "\n",
    "    #Invoking - Option 2\n",
    "    input_dict = {\n",
    "        \"preferences\": user_input[\"preferences\"],\n",
    "        \"context\": relevant_text,\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    }\n",
    "    chain = template | llm\n",
    "    output = chain.invoke(input_dict)\n",
    "\n",
    "    try:\n",
    "        result = parser.parse(output.content)\n",
    "    except OutputParserException:\n",
    "        return \"Could not parse output.\", None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c7cc8-b049-4f66-94d2-026a4bf7b157",
   "metadata": {},
   "source": [
    "## User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d688e9-2251-4e43-9635-e866b9a4e175",
   "metadata": {},
   "source": [
    "### Simple UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f83764-2089-490d-9d02-19d6d80f63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_interface(preferences, model_choice, temperature):\n",
    "    user_input = {\n",
    "        \"preferences\": preferences,\n",
    "        \"model_choice\": model_choice,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    result = generate_cart(model_choice, user_input)\n",
    "    if not result or isinstance(result, str):\n",
    "        return result, None\n",
    "    if isinstance(result, tuple):\n",
    "        explanation, _ = result\n",
    "        return explanation, None\n",
    "\n",
    "    # Build a gallery format: [(image_url, caption), ...]\n",
    "    gallery_items = [\n",
    "        (item.image_url, f\"{item.item_name}\\nQty.{item.quantity}\\n₹{item.quantity*item.price}\\n\")\n",
    "        for item in result.items\n",
    "    ]\n",
    "    return result.reasoning, gallery_items\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Describe your grocery needs (e.g., 'high protein, no besan or curd')\"),\n",
    "        gr.Dropdown(label=\"Model\", choices=model_choices),\n",
    "        gr.Slider(minimum=0.0, maximum=1.5, value=0.7, step=0.1, label=\"Temperature\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Considerations\"),\n",
    "        gr.Gallery(label=\"Shopping Cart\", columns=3, height=\"auto\")\n",
    "    ],\n",
    "    title=\"Smart Grocery Cart Assistant\",\n",
    "    description=\"Get a product list tailored to your dietary preferences.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730ccb2-0f79-4b03-b7a5-4019629964b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57b9dd-d530-47a5-a0d6-51c7edc89f76",
   "metadata": {},
   "source": [
    "### Better UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6778429-f3e0-481a-aa4e-d288cc5bdc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_interface(preferences, model_choice, temperature):\n",
    "    user_input = {\n",
    "        \"preferences\": preferences,\n",
    "        \"model_choice\": model_choice,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    result = generate_cart(model_choice, user_input)\n",
    "    if not result or isinstance(result, str):\n",
    "        return result, []\n",
    "    if isinstance(result, tuple):\n",
    "        explanation, _ = result\n",
    "        return explanation, []\n",
    "    gallery_items = [\n",
    "        (item.image_url, f\"{item.item_name}\\nQty.{item.quantity}\\n₹{item.quantity*item.price}\\n\")\n",
    "        for item in result.items\n",
    "    ]\n",
    "    return result.reasoning, gallery_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69afab14-756e-4b87-b320-81523402074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Blocks layout\n",
    "with gr.Blocks(title=\"Smart Grocery Cart Assistant\") as demo:\n",
    "    gr.Markdown(\"## Smart Grocery Cart Assistant\")\n",
    "    gr.Markdown(\"Enter your grocery preferences and let AI suggest a weekly cart!\")\n",
    "\n",
    "    with gr.Row():\n",
    "        preferences_input = gr.Textbox(\n",
    "            label=\"Describe your grocery needs\",\n",
    "            placeholder=\"e.g., high protein, no besan or curd\",\n",
    "            lines=2\n",
    "        )\n",
    "\n",
    "    with gr.Accordion(\"LLM Settings (Advanced)\", open=False):\n",
    "        model_dropdown = gr.Dropdown(\n",
    "            label=\"Model\",\n",
    "            choices=model_choices,\n",
    "            value=model_choices[0]\n",
    "        )\n",
    "        temperature_slider = gr.Slider(\n",
    "            minimum=0.0, maximum=1.5, value=0.7, step=0.1,\n",
    "            label=\"Temperature\"\n",
    "        )\n",
    "\n",
    "    run_button = gr.Button(\"Generate Cart\")\n",
    "\n",
    "    with gr.Row():\n",
    "        reasoning_output = gr.Textbox(label=\"Remarks\", lines=2)\n",
    "\n",
    "    with gr.Row():\n",
    "        cart_output = gr.Gallery(label=\"Shopping Cart\", columns=3, height=\"auto\")\n",
    "\n",
    "    run_button.click(\n",
    "        fn=gradio_interface,\n",
    "        inputs=[preferences_input, model_dropdown, temperature_slider],\n",
    "        outputs=[reasoning_output, cart_output]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d63bd-03c6-42eb-991a-c6c87ed914f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179b34e-ceae-4a95-8ec7-f26f36d6c862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
